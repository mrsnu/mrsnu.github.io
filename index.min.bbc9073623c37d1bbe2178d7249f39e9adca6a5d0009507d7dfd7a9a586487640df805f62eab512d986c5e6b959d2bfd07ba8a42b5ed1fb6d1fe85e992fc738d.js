var suggestions = document.getElementById('suggestions');
var search = document.getElementById('search');

if (search !== null) {
  document.addEventListener('keydown', inputFocus);
}

function inputFocus(e) {
  if (e.ctrlKey && e.key === '/' ) {
    e.preventDefault();
    search.focus();
  }
  if (e.key === 'Escape' ) {
    search.blur();
    suggestions.classList.add('d-none');
  }
}

document.addEventListener('click', function(event) {

  var isClickInsideElement = suggestions.contains(event.target);

  if (!isClickInsideElement) {
    suggestions.classList.add('d-none');
  }

});

/*
Source:
  - https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3
*/

document.addEventListener('keydown',suggestionFocus);

function suggestionFocus(e) {
  const suggestionsHidden = suggestions.classList.contains('d-none');
  if (suggestionsHidden) return;

  const focusableSuggestions= [...suggestions.querySelectorAll('a')];
  if (focusableSuggestions.length === 0) return;

  const index = focusableSuggestions.indexOf(document.activeElement);

  if (e.key === "ArrowUp") {
    e.preventDefault();
    const nextIndex = index > 0 ? index - 1 : 0;
    focusableSuggestions[nextIndex].focus();
  }
  else if (e.key === "ArrowDown") {
    e.preventDefault();
    const nextIndex= index + 1 < focusableSuggestions.length ? index + 1 : index;
    focusableSuggestions[nextIndex].focus();
  }

}

/*
Source:
  - https://github.com/nextapps-de/flexsearch#index-documents-field-search
  - https://raw.githack.com/nextapps-de/flexsearch/master/demo/autocomplete.html
*/

(function(){

  var index = new FlexSearch.Document({
    tokenize: "forward",
    cache: 100,
    document: {
      id: 'id',
      store: [
        "href", "title", "description"
      ],
      index: ["title", "description", "content"]
    }
  });


  // Not yet supported: https://github.com/nextapps-de/flexsearch#complex-documents

  /*
  var docs = [
    {
        id: 0,
        href: "https://mrsnu.github.io/docs/prologue/",
        title: "Prologue",
        description: "Prologue Doks.",
        content: ""
      },
    {
        id: 1,
        href: "https://mrsnu.github.io/docs/getting-started/",
        title: "Getting Started",
        description: "Getting Started",
        content: ""
      },
    {
        id: 2,
        href: "https://mrsnu.github.io/docs/api/",
        title: "API",
        description: "API",
        content: ""
      },
    {
        id: 3,
        href: "https://mrsnu.github.io/docs/prologue/introduction/",
        title: "Introduction",
        description: "",
        content: "\u003cp\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/3498361.3538948\"\u003eBand\u003c/a\u003e is an efficient deep learning platform for mobile-cloud collaborative support for multiple DNNs.\nBand supports backend-agnostic coordination of DNN requests on heterogeneous processors in a mobile device to \u003cs\u003ecloud server\u003c/s\u003e.\nBand is currently backed by following backend machine learning frameworks.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://github.com/tensorflow/tensorflow/tree/v2.9.2\"\u003eTensorflow v2.9.2\u003c/a\u003e\u003c/th\u003e\n\u003cth\u003e\u0026hellip;\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eAndroid\u003c/td\u003e\n\u003ctd\u003eâ˜‘\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eiOS\u003c/td\u003e\n\u003ctd\u003eâ˜\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egRPC\u003c/td\u003e\n\u003ctd\u003eâ˜\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eBand provides Java and C APIs, as well as an official plugin for \u003ca href=\"https://www.unrealengine.com/\"\u003eUnreal Engine\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"quick-start\"\u003eQuick Start \u003ca href=\"#quick-start\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cdiv class=\"alert alert-doks d-flex\" role=\"alert\"\u003e\n  \u003cdiv class=\"flex-shrink-1 alert-icon\"\u003eðŸ‘‰ \u003c/div\u003e\n  \n    \u003cdiv class=\"w-100\"\u003eThe Quick Start is intended for intermediate users. \u003c/div\u003e\n  \n\u003c/div\u003e\n\n\u003cp\u003eOne page summary of how to build Android Band application. \u003ca href=\"/docs/getting-started/quick-start/\"\u003eQuick Start â†’\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"examples\"\u003eExamples \u003ca href=\"#examples\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSee what others have build with Band. \u003ca href=\"/example/\"\u003eExample â†’\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"how-to-build-and-modify-band\"\u003eHow to build and modify Band \u003ca href=\"#how-to-build-and-modify-band\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cdiv class=\"alert alert-doks d-flex\" role=\"alert\"\u003e\n  \u003cdiv class=\"flex-shrink-1 alert-icon\"\u003eðŸ‘‰ \u003c/div\u003e\n  \n    \u003cdiv class=\"w-100\"\u003eThe Quick Start is intended for advanced users. \u003c/div\u003e\n  \n\u003c/div\u003e\n\n\u003cp\u003eOne page summary of how to build and customize Band. \u003ca href=\"/docs/getting-started/build-guide/\"\u003eBuild Guide â†’\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"about-us\"\u003eAbout Us \u003ca href=\"#about-us\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eFind out who we are. \u003ca href=\"/docs/prologue/about-us/\"\u003eAbout Us-\u0026gt;\u003c/a\u003e\u003c/p\u003e\n"
      },
    {
        id: 4,
        href: "https://mrsnu.github.io/docs/getting-started/quick-start/",
        title: "Quick Start",
        description: "One page summary of how to use a Band.",
        content: "\u003cp\u003eIn this guide, we will show you how to use Band to develop Android application.\u003c/p\u003e\n\u003ch3 id=\"steps-to-use-band\"\u003eSteps to use Band \u003ca href=\"#steps-to-use-band\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstall \u003ca href=\"https://developer.android.com/studio\"\u003eAndroid Studio\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClone Android example project and open \u003ccode\u003eband-example/android/object_detection\u003c/code\u003e it with Android Studio\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003egit clone https://github.com/mrsnu/band-example.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChange the application ID in \u003ccode\u003eband-example/android/object_detection/app/build.gradle\u003c/code\u003e to your own application ID\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-gradle\"\u003edefaultConfig {\n    applicationId \u0026quot;com.band.example\u0026quot;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUpdate the package name in \u003ccode\u003eband-example/android/object_detection/app/src/main/AndroidManifest.xml\u003c/code\u003e to your own package name\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-xml\"\u003e\u0026lt;manifest xmlns:android=\u0026quot;http://schemas.android.com/apk/res/android\u0026quot;\n    package=\u0026quot;com.band.example\u0026quot;\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eChange the application name in \u003ccode\u003eband-example/android/object_detection/app/src/main/res/values/strings.xml\u003c/code\u003e to your own application name\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-xml\"\u003e\u0026lt;resources\u0026gt;\n    \u0026lt;string name=\u0026quot;app_name\u0026quot;\u0026gt;Band Example\u0026lt;/string\u0026gt;\n\u0026lt;/resources\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart developing your own application!\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
    {
        id: 5,
        href: "https://mrsnu.github.io/docs/prologue/about-us/",
        title: "About Us",
        description: "Band is built and designed by the team `SNUMR` from Seoul National University Dept. of Computer Science and Engineering. We are cross-laboratory team from Software Platforms Lab and Human-Centered Computer-Systems Lab.",
        content: "\u003ch3 id=\"current-members\"\u003eCurrent Members \u003ca href=\"#current-members\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://youngkilee.blogspot.com/\"\u003eYoungki Lee\u003c/a\u003e,\n\u003ca href=\"https://bgchun.github.io/\"\u003eByung-Gon Chun\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jingyulee.com/\"\u003eJingyu Lee\u003c/a\u003e,\n\u003ca href=\"https://changminjeon.com/\"\u003eChangmin Jeon\u003c/a\u003e,\nMinjae Kim,\nHyunsoo Kim,\nSeonjun Kim\u003c/p\u003e\n\u003ch3 id=\"previous-members\"\u003ePrevious Members \u003ca href=\"#previous-members\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eJoo Seong Jeong,\nDonghyun Kim,\nChangjin Jeong\u003c/p\u003e\n\u003ch3 id=\"citation\"\u003eCitation \u003ca href=\"#citation\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIf you find our work useful, please cite our paper below!\nThe original codebase for paper submission is archived \u003ca href=\"https://github.com/mrsnu/band/releases/tag/v0.0.0\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bibtex\"\u003e@inproceedings{jeong2022band,\n  title={Band: coordinated multi-DNN inference on heterogeneous mobile processors},\n  author={Jeong, Joo Seong and Lee, Jingyu and Kim, Donghyun and Jeon, Changmin and Jeong, Changjin and Lee, Youngki and Chun, Byung-Gon},\n  booktitle={Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},\n  pages={235--247},\n  year={2022}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"acknowledgment\"\u003eAcknowledgment \u003ca href=\"#acknowledgment\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis work was supported by Samsung Research Funding \u0026amp; Incubation Center of Samsung Electronics under project number SRFC-IT2001-03.\u003c/p\u003e\n"
      },
    {
        id: 6,
        href: "https://mrsnu.github.io/docs/getting-started/benchmark/",
        title: "Benchmark Tool",
        description: "One page summary of how to use a Benchmark Tool for Band.",
        content: "\u003cp\u003eBand provides a simple C++ binary to benchmark a runtime performance.\nThe binary generates repeatitive model requests based on a given config file, and reports latency statistics afterwards.\u003c/p\u003e\n\u003ch2 id=\"how-to-run\"\u003eHow to run \u003ca href=\"#how-to-run\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003e[root]/script/run_benchmark.py\u003c/code\u003e script will build \u003ccode\u003eband_benchmark\u003c/code\u003e binary file and execute it with a specified config file. Built binary file and target config file can be found in \u003ccode\u003e[root]/benchmark\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"on-android\"\u003eOn Android \u003ca href=\"#on-android\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIf you want to build binary from docker container (Refer to \u003ccode\u003e[root]/script/docker_util.sh\u003c/code\u003e for more detail)\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython .\\script\\run_benchmark.py -android -docker -c .\\benchmark_config.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to build locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython .\\script\\run_benchmark.py -android -c .\\benchmark_config.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"on-local-desktop-windows-or-ubuntu\"\u003eOn local desktop (Windows or Ubuntu) \u003ca href=\"#on-local-desktop-windows-or-ubuntu\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython .\\script\\run_benchmark.py -c .\\benchmark_config.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"config-file\"\u003eConfig file \u003ca href=\"#config-file\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"structure\"\u003eStructure \u003ca href=\"#structure\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emodels\u003c/code\u003e: Models to run. For each model, specify the following fields.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egraph\u003c/code\u003e: Model path.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eperiod_ms\u003c/code\u003e: \u003cstrong\u003eOptional\u003c/strong\u003e The delay between subsequent requests in ms. The argument is only effective with \u003ccode\u003eperiodic\u003c/code\u003e execution mode.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ebatch_size\u003c/code\u003e: The number of model requests in a frame. [default: 1]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eworker_id\u003c/code\u003e: \u003cstrong\u003eOptional\u003c/strong\u003e Specify the worker id to run in int. The argument is only effective with \u003ccode\u003efixed_device\u003c/code\u003e scheduler.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eslo_us\u003c/code\u003e and \u003ccode\u003eslo_scale\u003c/code\u003e: \u003cstrong\u003eOptional\u003c/strong\u003e fields for specifying an SLO value for a model. Setting \u003ccode\u003eslo_scale\u003c/code\u003e will make the SLO = worst profiled latency of that model * \u003ccode\u003eslo_scale\u003c/code\u003e. \u003ccode\u003eslo_scale\u003c/code\u003e will be ignored if \u003ccode\u003eslo_us\u003c/code\u003e is given (i.e., no reason to specify both options).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elog_path\u003c/code\u003e: The log file path. (e.g., \u003ccode\u003e/data/local/tmp/model_execution_log.json\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eschedulers\u003c/code\u003e: The scheduler types in \u003ccode\u003elist[string]\u003c/code\u003e. If N schedulers are specified, then N queues are generated.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efixed_worker\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eround_robin\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eshortest_expected_latency\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eleast_slack_time_first\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eheterogeneous_earliest_finish_time\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eheterogeneous_earliest_finish_time_reserved\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eminimum_subgraph_size\u003c/code\u003e: Minimum subgraph size. If candidate subgraph size is smaller than \u003ccode\u003eminimum_subgraph_size\u003c/code\u003e, the subgraph will not be created. [default: 7]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esubgraph_preparation_type\u003c/code\u003e: For schedulers using fallback, determine how to generate candidate subgraphs. [default: \u003ccode\u003emerge_unit_subgraph\u003c/code\u003e]\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eno_fallback_subgraph\u003c/code\u003e: Generate subgraphs per worker. Explicit fallback subgraph will not be generated.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efallback_per_worker\u003c/code\u003e: Generate fallback subgraphs for each worker.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eunit_subgraph\u003c/code\u003e: Generate unit subgraphs considering all device supportiveness. All ops in same unit subgraph have same support devices.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emerge_unit_subgraph\u003c/code\u003e: Add merged unit subgraphs to \u003ccode\u003eunit_subgraph\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eexecution_mode\u003c/code\u003e: Specify a exeucution mode. Available execution modes are as follows:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003estream\u003c/code\u003e: consecutively run batches.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eperiodic\u003c/code\u003e: invoke requests periodically.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eworkload\u003c/code\u003e: execute pre-defined sequence in \u003ccode\u003estream\u003c/code\u003e manner based on a given workload file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecpu_masks\u003c/code\u003e: CPU cluster mask to set CPU affinity. [default: \u003ccode\u003eALL\u003c/code\u003e]\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eALL\u003c/code\u003e: All Cluster\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLITTLE\u003c/code\u003e: LITTLE Cluster only\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBIG\u003c/code\u003e: Big Cluster only\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ePRIMARY\u003c/code\u003e: Primary Core only\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_threads\u003c/code\u003e: Number of computing threads for CPU delegates. [default: -1]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eplanner_cpu_masks\u003c/code\u003e: CPU cluster mask to set CPU affinity of planner. [default: same value as global \u003ccode\u003ecpu_masks\u003c/code\u003e]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eworkers\u003c/code\u003e: A vector-like config for per-processor worker. For each worker, specify the following fields. System creates 1 worker per device by default and first provided value overrides the settings (i.e., \u003ccode\u003ecpu_masks\u003c/code\u003e, \u003ccode\u003enum_threads\u003c/code\u003e, \u003ccode\u003eprofile_copy_computation_ratio\u003c/code\u003e, \u0026hellip; ) and additional field will add additional worker per device.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edevice\u003c/code\u003e: Target device of specific worker.\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eCPU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eGPU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDSP\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eNPU\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecpu_masks\u003c/code\u003e: CPU cluster mask to set CPU affinity of specific worker. [default: same value as global \u003ccode\u003ecpu_masks\u003c/code\u003e]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_threads\u003c/code\u003e: Number of threads. [default: same value as global \u003ccode\u003enum_threads\u003c/code\u003e]\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erunning_time_ms\u003c/code\u003e: Experiment duration in ms. [default: 60000]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_smoothing_factor\u003c/code\u003e: Current profile reflection ratio. \u003ccode\u003eupdated_profile = profile_smoothing_factor * curr_profile + (1 - profile_smoothing_factor) * prev_profile\u003c/code\u003e [default: 0.1]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emodel_profile\u003c/code\u003e: The path to file with model profile results. [default: None]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_online\u003c/code\u003e: Online profile or offline profile [default: true]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_warmup_runs\u003c/code\u003e: Number of warmup runs before profile. [default: 1]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_num_runs\u003c/code\u003e: Number of runs for profile. [default: 1]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_copy_computation_ratio\u003c/code\u003e: Ratio of computation / input-ouput copy in \u003ccode\u003elist[int]\u003c/code\u003e. Used for latency estimation for each device type (e.g., CPU, GPU, DSP, NPU). The length of the list should be equal to the 4 (\u003ccode\u003eGetSize\u0026lt;DeviceFlags\u0026gt;()\u003c/code\u003e). [default: 30000, 30000, 30000, 30000]\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eschedule_window_size\u003c/code\u003e: The number of planning unit.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eworkload\u003c/code\u003e: The path to file with workload information. [default: None]\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"example\"\u003eExample \u003ca href=\"#example\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-json\"\u003e{\n    \u0026quot;models\u0026quot;: [\n        {\n            \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/lite-model_efficientdet_lite0_int8_1.tflite\u0026quot;,\n            \u0026quot;period_ms\u0026quot;: 30,\n            \u0026quot;batch_size\u0026quot;: 3\n        },\n        {\n            \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/retinaface_mbv2_quant_160.tflite\u0026quot;,\n            \u0026quot;period_ms\u0026quot;: 30,\n            \u0026quot;batch_size\u0026quot;: 3\n        },\n        {\n            \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/ssd_mobilenet_v1_1_metadata_1.tflite\u0026quot;,\n            \u0026quot;period_ms\u0026quot;: 30,\n            \u0026quot;batch_size\u0026quot;: 3\n        }\n    ],\n    \u0026quot;log_path\u0026quot;: \u0026quot;/data/local/tmp/log.json\u0026quot;,\n    \u0026quot;schedulers\u0026quot;: [\n        \u0026quot;heterogeneous_earliest_finish_time_reserved\u0026quot;\n    ],\n    \u0026quot;minimum_subgraph_size\u0026quot;: 7,\n    \u0026quot;subgraph_preparation_type\u0026quot;: \u0026quot;merge_unit_subgraph\u0026quot;,\n    \u0026quot;execution_mode\u0026quot;: \u0026quot;stream\u0026quot;,\n    \u0026quot;cpu_masks\u0026quot;: \u0026quot;ALL\u0026quot;,\n    \u0026quot;num_threads\u0026quot;: 1,\n    \u0026quot;planner_cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot;,\n    \u0026quot;workers\u0026quot;: [\n        {\n            \u0026quot;device\u0026quot;: \u0026quot;CPU\u0026quot;,\n            \u0026quot;num_threads\u0026quot;: 1,\n            \u0026quot;cpu_masks\u0026quot;: \u0026quot;BIG\u0026quot;\n        },\n        {\n            \u0026quot;device\u0026quot;: \u0026quot;CPU\u0026quot;,\n            \u0026quot;num_threads\u0026quot;: 1,\n            \u0026quot;cpu_masks\u0026quot;: \u0026quot;LITTLE\u0026quot;\n        },\n        {\n            \u0026quot;device\u0026quot;: \u0026quot;GPU\u0026quot;,\n            \u0026quot;num_threads\u0026quot;: 1,\n            \u0026quot;cpu_masks\u0026quot;: \u0026quot;ALL\u0026quot;\n        },\n        {\n            \u0026quot;device\u0026quot;: \u0026quot;DSP\u0026quot;,\n            \u0026quot;num_threads\u0026quot;: 1,\n            \u0026quot;cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot;\n        },\n        {\n            \u0026quot;device\u0026quot;: \u0026quot;NPU\u0026quot;,\n            \u0026quot;num_threads\u0026quot;: 1,\n            \u0026quot;cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot;\n        }\n    ],\n    \u0026quot;running_time_ms\u0026quot;: 10000,\n    \u0026quot;profile_smoothing_factor\u0026quot;: 0.1,\n    \u0026quot;profile_data_path\u0026quot;: \u0026quot;/data/local/tmp/profile.json\u0026quot;,\n    \u0026quot;profile_online\u0026quot;: true,\n    \u0026quot;profile_warmup_runs\u0026quot;: 3,\n    \u0026quot;profile_num_runs\u0026quot;: 50,\n    \u0026quot;profile_copy_computation_ratio\u0026quot;: [\n        1000,\n        1000,\n        1000,\n        1000\n    ],\n    \u0026quot;availability_check_interval_ms\u0026quot;: 30000,\n    \u0026quot;schedule_window_size\u0026quot;: 10\n}\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
    {
        id: 7,
        href: "https://mrsnu.github.io/docs/getting-started/config/",
        title: "Config",
        description: "Configuration and its Builder for Band Runtime.",
        content: "\u003cp\u003eThere are four configuration objects for each Band\u0026rsquo;s component.\u003c/p\u003e\n\u003cp\u003eEach configuration field is optional or required. If a field is optional, then it is guaranteed that the default value exists. If a field is required, a configuration cannot be generated by \u003ccode\u003eRuntimeConfigBuilder\u003c/code\u003e without specifying the field.\u003c/p\u003e\n\u003ch3 id=\"enumeration-types\"\u003eEnumeration Types \u003ca href=\"#enumeration-types\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eBandSchedulerType\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekFixedDeviceFixedWorker\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekRoundRobin\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekShortestExpectedLatency\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekFixedDeviceGlobalQueue\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekHeterogeneousEarliestFinishTime\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekLeastSlackTimeFirst\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekHeterogeneousEarliestFinishTimeReserved\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eCPUMaskFlags\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekAll\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekLittle\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBig\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekPrimary\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eDeviceFlags\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekCPU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekGPU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekDSP\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekNPU\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSubgraphPreparationType\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekNoFallbackSubgraph\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekFallbackPerWorker\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekUnitSubgraph\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekMergeUnitSubgraph\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"profileconfig\"\u003e\u003ccode\u003eProfileConfig\u003c/code\u003e \u003ca href=\"#profileconfig\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eonline\u003c/code\u003e [type: \u003ccode\u003ebool\u003c/code\u003e, default: \u003ccode\u003etrue\u003c/code\u003e]: Profile online if true, offline if false.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_warmups\u003c/code\u003e [type: \u003ccode\u003eint\u003c/code\u003e, default: \u003ccode\u003e1\u003c/code\u003e]: The number of warmup runs before profile.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_runs\u003c/code\u003e [type: \u003ccode\u003eint\u003c/code\u003e, default: \u003ccode\u003e1\u003c/code\u003e]: The number of runs for profile\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecopy_computation_ratio\u003c/code\u003e [type: \u003ccode\u003estd::vector\u0026lt;int\u0026gt;\u003c/code\u003e, default: \u003ccode\u003e[30000, ...]\u003c/code\u003e]: The ratio of computation to input-output copy. Used for latency estimation. The size of the list should be the same as the number of devices.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esmoothing_factor\u003c/code\u003e [type: \u003ccode\u003efloat\u003c/code\u003e, default: \u003ccode\u003e0.1\u003c/code\u003e]: The momentum to reflect current profiled data. \u003ccode\u003e\u0026lt;updateed_profile\u0026gt; = \u0026lt;smoothing_factor\u0026gt; * \u0026lt;curr_profile\u0026gt; + (1. - \u0026lt;smoothing_factor\u0026gt;) * \u0026lt;prev_profile\u0026gt;\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprofile_data_path\u003c/code\u003e [type: \u003ccode\u003estd::string\u003c/code\u003e, default: \u003ccode\u003e\u0026quot;\u0026quot;\u003c/code\u003e]: The input path to the file for offline profile results. If not specified, this will be ignored and will not generate the result file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"plannerconfig\"\u003e\u003ccode\u003ePlannerConfig\u003c/code\u003e \u003ca href=\"#plannerconfig\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eschedule_window_size\u003c/code\u003e [type: \u003ccode\u003eint\u003c/code\u003e, default: \u003ccode\u003eINT_MAX\u003c/code\u003e]: The size of window that scheduler will use.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eschedulers\u003c/code\u003e [type: \u003ccode\u003estd::vector\u0026lt;SchedulerType\u0026gt;\u003c/code\u003e, \u003cstrong\u003erequired\u003c/strong\u003e]: The types of schedulers. If \u003ccode\u003eN\u003c/code\u003e schedulers are specified, \u003ccode\u003eN\u003c/code\u003e queues will be generated.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecpu_mask\u003c/code\u003e [type: \u003ccode\u003eCPUMaskFlags\u003c/code\u003e, default: \u003ccode\u003ekAll\u003c/code\u003e]: CPU masks to set CPU affinity.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elog_path\u003c/code\u003e [type: \u003ccode\u003estd::string\u003c/code\u003e, default: \u003ccode\u003e\u0026quot;\u0026quot;\u003c/code\u003e]: The output path to the file for planner\u0026rsquo;s log. If not specified, this will be ignored and will not generate the result file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"workerconfig\"\u003e\u003ccode\u003eWorkerConfig\u003c/code\u003e \u003ca href=\"#workerconfig\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eworkers\u003c/code\u003e [type: \u003ccode\u003estd::vector\u0026lt;DeviceFlags\u0026gt;\u003c/code\u003e, default: \u003ccode\u003e[kCPU, kGPU, ...]\u003c/code\u003e]: The list of target devices. By default, one worker per device is generated.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecpu_masks\u003c/code\u003e [type: \u003ccode\u003estd::vector\u0026lt;CPUMaskFlags\u0026gt;\u003c/code\u003e, default: \u003ccode\u003e[kAll, kAll, ...]\u003c/code\u003e]: CPU masks to set CPU affinity. The size of the list must be the same as the size of \u003ccode\u003eworkers\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_threads\u003c/code\u003e [type: \u003ccode\u003estd::vector\u0026lt;int\u0026gt;\u003c/code\u003e, default: \u003ccode\u003e[1, 1, ...]\u003c/code\u003e]: The number of threads. The size of the list must be the same as the size of \u003ccode\u003eworkers\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eallow_worksteal\u003c/code\u003e [type: \u003ccode\u003ebool\u003c/code\u003e, default: \u003ccode\u003efalse\u003c/code\u003e]: Work-stealing is enabled if true, disabled if false.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eavailability_check_interval_ms\u003c/code\u003e [type: \u003ccode\u003eint\u003c/code\u003e, default: \u003ccode\u003e30_000\u003c/code\u003e]: The interval for checking availability of devices. Used for detecting thermal throttling.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"runtimeconfig\"\u003e\u003ccode\u003eRuntimeConfig\u003c/code\u003e \u003ca href=\"#runtimeconfig\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eRuntimeConfig\u003c/code\u003e contains \u003ccode\u003eProfileConfig\u003c/code\u003e, \u003ccode\u003ePlannerConfig\u003c/code\u003e and \u003ccode\u003eWorkerConfig\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eminimum_subgraph_size\u003c/code\u003e [type: \u003ccode\u003eint\u003c/code\u003e, default: \u003ccode\u003e7\u003c/code\u003e]: The minimum subgraph size. If candidate subgraph size is smaller than this, the subgraph will not be created.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esubgraph_preparation_type\u003c/code\u003e [type: \u003ccode\u003eSubgraphPreparationType\u003c/code\u003e, default: \u003ccode\u003ekMergeUnitSubgraph\u003c/code\u003e]: For fallback schedulers, determine how to generate candidate subgraphs.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecpu_mask\u003c/code\u003e [type: \u003ccode\u003eCPUMaskFlags\u003c/code\u003e, default: \u003ccode\u003ekAll\u003c/code\u003e]: The CPU mask for Band Engine.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"runtimeconfigbuilder-api\"\u003e\u003ccode\u003eRuntimeConfigBuilder\u003c/code\u003e API \u003ca href=\"#runtimeconfigbuilder-api\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eRuntimeConfigBuilder\u003c/code\u003e delegates all builder that inherits \u003ccode\u003eConfigBuilder\u003c/code\u003e.\nIt is a \u003ccode\u003efriend\u003c/code\u003e class of all the other \u003ccode\u003eConfigBuilder\u003c/code\u003e classes, so make sure to not change their members in \u003ccode\u003eRuntimeConfigBuilder\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"exmaple-usage\"\u003eExmaple Usage \u003ca href=\"#exmaple-usage\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c++\"\u003eRuntimeConfigBuilder b;\nauto config = b.AddOnline(false)  // Default was `true`\n                  .AddSmoothingFactor(0.3)  // Default was `0.1`\n                  .AddSchedulers({SchedulerType::kRoundRobin, SchedulerType::kLeastSlackTimeFirst})  // Required field.\n                  .Build();\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"methods\"\u003eMethods \u003ca href=\"#methods\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAll \u003ccode\u003eAdd*\u003c/code\u003e methods are idempotent, i.e. multiple calls behaves the same as a single call.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eAddOnline(bool online)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddNumWarmups(int num_warmups)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddNumRuns(int num_runs)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddCopyComputationRatio(std::vector\u0026lt;int\u0026gt; copy_computation_ratio)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddSmoothingFactor(float smoothing_factor)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddProfileLogPath(std::string profile_data_path)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddPlannerLogPath(std::string planner_log_path)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddScheduleWindowSize(int schedule_window_size)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddSchedulers(std::vector\u0026lt;SchedulerType\u0026gt; schedulers)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddPlannerCPUMask(CPUMaskFlags cpu_masks)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddWorkers(std::vector\u0026lt;DeviceFlags\u0026gt; workers)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddWorkerCPUMasks(std::vector\u0026lt;CPUMaskFlags\u0026gt; cpu_masks)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddWorkerNumThreads(std::vector\u0026lt;int\u0026gt; num_threads)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddAllowWorkSteal(bool allow_worksteal)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddAvailabilityCheckIntervalMs(int32_t availability_check_interval_ms)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddMinimumSubgraphSize(int minimum_subgraph_size)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddSubgraphPreparationType(SubgraphPreparationType subgraph_preparation_type)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAddCPUMask(CPUMaskFlags cpu_mask)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
    {
        id: 8,
        href: "https://mrsnu.github.io/docs/getting-started/preprocessing/",
        title: "Preprocessing",
        description: "Buffer and its Builder for Band Runtime.",
        content: "\u003ch2 id=\"overview\"\u003eOverview \u003ca href=\"#overview\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides a set of APIs to preprocess the data. The preprocessing is mendatory and time-consuming process to run the machine learning model on the Band. Band provides \u003ccode\u003eBuffer\u003c/code\u003e and \u003ccode\u003eBufferProcessor\u003c/code\u003e to efficiently develop the preprocessing pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eBuffer\u003c/code\u003e is an arbitrary wrapper of the data. It wraps the data with the metadata such as image, text, and Band \u003ccode\u003eTensor\u003c/code\u003e to be used in the preprocessing pipeline. \u003ccode\u003eBufferProcessor\u003c/code\u003e is a set of APIs to preprocess the data. It provides the basic preprocessing APIs such as \u003ccode\u003eresize\u003c/code\u003e, \u003ccode\u003enormalize\u003c/code\u003e, and \u003ccode\u003ecrop\u003c/code\u003e. We currently support image preprocessing only but we will support other data types such as text and audio in the future.\u003c/p\u003e\n\u003ch2 id=\"example-usage---imageprocessor\"\u003eExample Usage - ImageProcessor \u003ca href=\"#example-usage---imageprocessor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBelow example shows how to use \u003ccode\u003eBufferProcessor\u003c/code\u003e to preprocess the image data. The example creates a \u003ccode\u003eBuffer\u003c/code\u003e from raw RGB data with (width, height) dimentions and (3) channels. Then, it creates an \u003ccode\u003eImageProcessor\u003c/code\u003e with \u003ccode\u003eResize\u003c/code\u003e and \u003ccode\u003eNormalize\u003c/code\u003e operations. It preprocesses the \u003ccode\u003eBuffer\u003c/code\u003e and updates a \u003ccode\u003eTensor\u003c/code\u003e with (224, 224) dimentions and (3) channels with \u003ccode\u003ekFloat32\u003c/code\u003e data type with the data normalized with (127.5, 127.5).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003eTensor* tensor = ... // Create a tensor\n\n// Create a buffer\nunsigned char* data = new unsigned char[width * height * 3];\ndata = ... // Fill the data\nBuffer* buffer = Buffer::CreateFromRaw(data, width, height, 3, BufferFormat::kRGB, DataType::kUInt8);\n\n// Create an image processor\nImageProcessorBuilder builder;\nbuilder.SetResize(224, 224);\nbuilder.SetNormalize(127.5f, 127.5f);\nabsl::StatusOr\u0026lt;std::unique_ptr\u0026lt;BufferProcessor\u0026gt;\u0026gt; preprocessor =\n      preprocessor_builder.Build();\n\n// Preprocess the buffer\npreprocessor-\u0026gt;process(*buffer, *tensor);\n... // Use the tensor\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"buffer\"\u003eBuffer \u003ca href=\"#buffer\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eBuffer\u003c/code\u003e can be created from following data types and metadata:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eraw data, width, height, \u003ccode\u003eBufferFormat\u003c/code\u003e, \u003ccode\u003eDataType\u003c/code\u003e, and \u003ccode\u003eBufferOrientation\u003c/code\u003e (\u003ccode\u003eBufferFormat::kGrayScale\u003c/code\u003e, \u003ccode\u003eBufferFormat::kRGB\u003c/code\u003e, \u003ccode\u003eBufferFormat::kRGBA\u003c/code\u003e, and \u003ccode\u003eBufferFormat::kRaw\u003c/code\u003e only)\u003c/li\u003e\n\u003cli\u003ey plane, u plane, v plane, width, height, raw stride of y plane, raw stride of uv plane, pixel stride of uv plane, \u003ccode\u003eBufferFormat\u003c/code\u003e, \u003ccode\u003eDataType\u003c/code\u003e, and \u003ccode\u003eBufferOrientation\u003c/code\u003e (\u003ccode\u003eBufferFormat::kYV12\u003c/code\u003e, \u003ccode\u003eBufferFormat::kYV21\u003c/code\u003e, \u003ccode\u003eBufferFormat::kNV21\u003c/code\u003e, and \u003ccode\u003eBufferFormat::kNV12\u003c/code\u003e only)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eTensor\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCurrently, \u003ccode\u003eBufferFormat\u003c/code\u003es that are not \u003ccode\u003ekRaw\u003c/code\u003e only support \u003ccode\u003ekUInt8\u003c/code\u003e \u003ccode\u003eDataType\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"enumeration-types\"\u003eEnumeration Types \u003ca href=\"#enumeration-types\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eBufferFormat\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekGrayScale\u003c/code\u003e - 8-bit gray scale\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekRGB\u003c/code\u003e - 8-bit RGB\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekRGBA\u003c/code\u003e - 8-bit RGBA\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekNV21\u003c/code\u003e - YUV 4:2:0, 8 bit per channel, interleaved\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekNV12\u003c/code\u003e - YUV 4:2:0, 8 bit per channel, interleaved\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekYV12\u003c/code\u003e - YUV 4:2:0, 8 bit per channel, planar\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekYV21\u003c/code\u003e - YUV 4:2:0, 8 bit per channel, planar\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekRaw\u003c/code\u003e - raw data\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eDataType\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekNoType\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekFloat32\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekInt32\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekUInt8\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekInt64\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekString\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBool\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekInt16\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekComplex64\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekInt8\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekFloat16\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekFloat64\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"bufferprocessor\"\u003eBufferProcessor \u003ca href=\"#bufferprocessor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"imageprocessor\"\u003eImageProcessor \u003ca href=\"#imageprocessor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eImageProcessor\u003c/code\u003e supports following operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eCrop(int x0, int y0, int x1, int y1)\u003c/code\u003e: crop from top-left corner, inclusive\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eResize(int width, int height)\u003c/code\u003e: resize to a new size\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eRotate(float angle)\u003c/code\u003e: counter-clockwise, between 0 and 360 in multiples of 90\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eFlip(bool horizontal, bool vertical)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eConvertColorSpace(BufferFormat target_format)\u003c/code\u003e: convert the color space\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eNormalize(float mean, float std)\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDataTypeConvert()\u003c/code\u003e: convert the data type to the output data type, e.g., convert from 8-bit RGB to 32-bit float RGB (tensor).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eImageProcessorBuilder\u003c/code\u003e provides a simple way to create an \u003ccode\u003eImageProcessor\u003c/code\u003e. The user predefines the operations and \u003ccode\u003eImageProcessorBuilder\u003c/code\u003e will create an \u003ccode\u003eImageProcessor\u003c/code\u003e with the operations.\u003c/p\u003e\n\u003cp\u003eBy default, \u003ccode\u003eImageProcessorBuilder\u003c/code\u003e without any operation will create a \u003ccode\u003eImageProcessor\u003c/code\u003e provides a direct mapping from entire \u003ccode\u003eBuffer\u003c/code\u003e to \u003ccode\u003eTensor\u003c/code\u003e without normalization. This covers the most common use case of the preprocessing.\u003c/p\u003e\n"
      },
    {
        id: 9,
        href: "https://mrsnu.github.io/docs/getting-started/build-guide/",
        title: "Build Guide (Advanced Users)",
        description: "One page summary of how to build a Band.",
        content: "\u003cp\u003eWe provide pre-built binaries for Android and Windows. However, if you want to build Band from source code, please follow the instructions below. It is currently tested on Ubuntu 18.04 and Windows 10.\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites \u003ca href=\"#prerequisites\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe recommend using \u003ca href=\"https://code.visualstudio.com/\"\u003eVisual Studio Code\u003c/a\u003e dev container to build Band. We provide the container to build Band without installing any dependencies. For more details, please refer to \u003ca href=\"https://code.visualstudio.com/docs/devcontainers/containers\"\u003eVisual Studio Code Dev Container\u003c/a\u003e. If you use the container, you can skip to \u003ca href=\"#prerequisites-for-android\"\u003ePrerequisites for Android\u003c/a\u003e and \u003ca href=\"#how-to-build--run\"\u003eHow to build \u0026amp; run\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eClone Band repository\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003egit clone https://github.com/mrsnu/band.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall submodules\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003ecd band\ngit submodule update --init --recursive\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall Bazel\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstall \u003ca href=\"https://github.com/bazelbuild/bazelisk\"\u003eBazelisk\u003c/a\u003e or \u003ca href=\"https://docs.bazel.build/versions/master/install.html\"\u003eBazel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eWe recommend to use Bazelisk to avoid version mismatch\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"prerequisites-for-android\"\u003ePrerequisites for Android \u003ca href=\"#prerequisites-for-android\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eConfigure Android SDK, NDK\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython configure.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"how-to-build--run\"\u003eHow to build \u0026amp; run \u003ca href=\"#how-to-build--run\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe provide useful scripts to build and run Band. Please refer to \u003ccode\u003e[root]/script\u003c/code\u003e for more details. Test and Benchmark script allows you to cross-compile from the docker container (from our dev container) on the host machine\u0026rsquo;s docker context and download the built binary to the host machine (\u003ccode\u003e-docker\u003c/code\u003e option). If you want to build locally, please remove \u003ccode\u003e-docker\u003c/code\u003e option.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRun test - Our test script will build and run tests for all platforms (Android, Linux, Windows). If not specified, it will build and run tests for the native platform (Linux or Windows) that executes the script.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython script/run_test.py -android -docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Android AAR - Our build script will build AAR for Android and copy it to \u003ccode\u003e[root]\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003esh script/build_aar_armv8.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild \u003ca href=\"/docs/api/c/\"\u003eC API\u003c/a\u003e - Our build script will build C API for Android, and output files are located at \u003ccode\u003e[root]/bazel-bin/band/c/band_c_pkg.tar\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython script/build_c_api.py -android \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun \u003ca href=\"/docs/getting-started/benchmark/\"\u003eBenchmark\u003c/a\u003e - Our benchmark script will build and run the benchmark for all platforms (Android, Linux, Windows). If not specified, it will build and run the benchmark for native platforms (Linux or Windows) that execute the script.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-sh\"\u003epython script/run_benchmark.py --config band/test/data/benchmark_config.json -android -docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
    {
        id: 10,
        href: "https://mrsnu.github.io/docs/api/c/",
        title: "C",
        description: "One page summary of how to use a C API.",
        content: "\u003ch2 id=\"introduction\"\u003eIntroduction \u003ca href=\"#introduction\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides a C API for developers who want to use Band in their C/C++ projects. The API is a thin wrapper around the core C++ API. The C API is available in the \u003ccode\u003elibband.h\u003c/code\u003e header file.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mrsnu/band/blob/master/band/c/example/band_c_main.c\"\u003eLink\u003c/a\u003e provides a complete example of how to dynamically load the library and use the C API.\u003c/p\u003e\n\u003ch3 id=\"example\"\u003eExample \u003ca href=\"#example\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003e#include \u0026lt;libband.h\u0026gt;\n\nint main() {\n  // 1. Create a configuration for the engine.\n  BandConfigBuilder* b = BandConfigBuilderCreate();\n  BandAddConfig(b, BAND_PLANNER_LOG_PATH, /*count=*/1, \u0026quot;log.json\u0026quot;);\n  BandAddConfig(b, BAND_PLANNER_SCHEDULERS, /*count=*/1, kBandHeterogeneousEarliestFinishTime);\n  BandConfig* config = BandConfigCreate(b);\n\n  // 2. Create an engine.\n  BandEngine* engine = BandEngineCreate(config);\n\n  // 3. Create and register a model.\n  BandModel* model = BandModelCreate();\n  BandModelAddFromFile(model, kBandTfLite,\n                        \u0026quot;mobilenet_v2_1.0_224_quant.tflite\u0026quot;);\n  BandEngineRegisterModel(engine, model);\n\n  // 4. Create input and output tensors for the model.\n  BandTensor* input_tensor = BandEngineCreateInputTensor(engine, model, 0);\n  BandTensor* output_tensor = BandEngineCreateOutputTensor(engine, model, 0);\n  std::tuple\u0026lt;unsigned char*, int, int\u0026gt; image_buffer =\n      LoadRGBImageRaw(\u0026quot;cat.jpg\u0026quot;);\n\n  // 5. (Optional) Create a buffer and an image processor to initialize the\n  // input tensor with the image data.\n  BandBuffer* buffer = BandBufferCreate();\n  BandBufferSetFromRawData(buffer, std::get\u0026lt;0\u0026gt;(image_buffer),\n                            std::get\u0026lt;1\u0026gt;(image_buffer), std::get\u0026lt;2\u0026gt;(image_buffer),\n                            kBandRGB);\n\n  BandImageProcessorBuilder* builder = BandImageProcessorBuilderCreate();\n  BandImageProcessor* processor = BandImageProcessorBuilderBuild(builder);\n  BandImageProcessorProcess(processor, buffer, input_tensor);\n\n  // 6. Run the model.\n  BandEngineRequestSync(engine, model, \u0026amp;input_tensor, \u0026amp;output_tensor);\n  \n  // 7. Get the result (class index).\n  unsigned char* output =\n      static_cast\u0026lt;unsigned char*\u0026gt;(BandTensorGetData(output_tensor));\n  // should be 282 (tiger cat) for cat.jpg\n  size_t class_index = ArgMax\u0026lt;unsigned char\u0026gt;(output, 1001);\n\n  // 8. Clean up.\n  BandTensorDelete(input_tensor);\n  BandTensorDelete(output_tensor);\n\n  BandImageProcessorBuilderDelete(builder);\n  BandImageProcessorDelete(processor);\n  delete[] std::get\u0026lt;0\u0026gt;(image_buffer);\n\n  BandEngineDelete(engine);\n  BandConfigDelete(config);\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"api-types\"\u003eAPI Types \u003ca href=\"#api-types\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides the following types in the C API:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eBandConfig\u003c/code\u003e: Configuration object for the engine.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandConfigBuilder\u003c/code\u003e: Builder for the configuration object.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandEngine\u003c/code\u003e: Engine object.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandModel\u003c/code\u003e: Model object that holds the model data. \u003cem\u003eIt must outlive the engine.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandTensor\u003c/code\u003e: Tensor object that holds the input/output data of a model.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandBuffer\u003c/code\u003e: Wrapper for any data buffer interchangable with \u003ccode\u003eBandTensor\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandImageProcessor\u003c/code\u003e: Image processor object that converts an image buffer to a tensor.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandImageProcessorBuilder\u003c/code\u003e: Builder for the image processor object.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBandRequestHandle\u003c/code\u003e: Handle for an asynchronous request.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"api-functions-engine\"\u003eAPI Functions (Engine) \u003ca href=\"#api-functions-engine\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"bandenginecreate\"\u003eBandEngineCreate \u003ca href=\"#bandenginecreate\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandEngine* BandEngineCreate(BandConfig* config);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a BandEngine instance with the given configuration.\u003c/p\u003e\n\u003ch3 id=\"bandenginedelete\"\u003eBandEngineDelete \u003ca href=\"#bandenginedelete\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandEngineDelete(BandEngine* engine);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDeletes the BandEngine instance.\u003c/p\u003e\n\u003ch3 id=\"bandengineregistermodel\"\u003eBandEngineRegisterModel \u003ca href=\"#bandengineregistermodel\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandEngineRegisterModel(BandEngine* engine, BandModel* model);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRegisters a model to the engine. The engine will load the model and allocate resources for it.\u003c/p\u003e\n\u003ch3 id=\"bandengingegetnuminputtensors\"\u003eBandEngingeGetNumInputTensors \u003ca href=\"#bandengingegetnuminputtensors\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eint BandEngingeGetNumInputTensors(BandEngine* engine, BandModel* model);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of input tensors for the given model.\u003c/p\u003e\n\u003ch3 id=\"bandengingegetnumoutputtensors\"\u003eBandEngingeGetNumOutputTensors \u003ca href=\"#bandengingegetnumoutputtensors\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eint BandEngingeGetNumOutputTensors(BandEngine* engine, BandModel* model);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of output tensors for the given model.\u003c/p\u003e\n\u003ch3 id=\"bandenginegetnumworkers\"\u003eBandEngineGetNumWorkers \u003ca href=\"#bandenginegetnumworkers\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eint BandEngineGetNumWorkers(BandEngine* engine);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of workers in the engine.\u003c/p\u003e\n\u003ch3 id=\"bandenginegetworkerdevice\"\u003eBandEngineGetWorkerDevice \u003ca href=\"#bandenginegetworkerdevice\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandDeviceFlag BandEngineGetWorkerDevice(BandEngine* engine, int worker_index);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the device flag (e.g., \u003ccode\u003ekBandCPU\u003c/code\u003e, \u003ccode\u003ekBandGPU\u003c/code\u003e, \u0026hellip;) of the worker at the given index.\u003c/p\u003e\n\u003ch3 id=\"bandenginecreateinputtensor\"\u003eBandEngineCreateInputTensor \u003ca href=\"#bandenginecreateinputtensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandTensor* BandEngineCreateInputTensor(BandEngine* engine, BandModel* model,\n                                        int index);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an input tensor for the given model. The tensor is allocated by the engine and must be deleted by the caller.\u003c/p\u003e\n\u003ch3 id=\"bandenginecreateoutputtensor\"\u003eBandEngineCreateOutputTensor \u003ca href=\"#bandenginecreateoutputtensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandTensor* BandEngineCreateOutputTensor(BandEngine* engine, BandModel* model,\n                                         int index);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an output tensor for the given model. The tensor is allocated by the engine and must be deleted by the caller.\u003c/p\u003e\n\u003ch3 id=\"bandenginerequestsync\"\u003eBandEngineRequestSync \u003ca href=\"#bandenginerequestsync\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandStatus BandEngineRequestSync(\n    BandEngine* engine, BandModel* model, BandTensor** input_tensors,\n    BandTensor** output_tensors)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuns the model with the given input tensors and stores the result in the output tensors. The function blocks until the execution is finished. Returns \u003ccode\u003ekBandOk\u003c/code\u003e if the execution is successful.\u003c/p\u003e\n\u003ch3 id=\"bandenginerequestasync\"\u003eBandEngineRequestAsync \u003ca href=\"#bandenginerequestasync\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandRequestHandle BandEngineRequestAsync(\n    BandEngine* engine, BandModel* model, BandTensor** input_tensors);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuns the model with the given input tensors. The function returns immediately and the result will be stored in the output tensors when the execution is finished. Returns a handle to the request.\u003c/p\u003e\n\u003ch3 id=\"bandenginewait\"\u003eBandEngineWait \u003ca href=\"#bandenginewait\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandStatus BandEngineWait(BandEngine* engine, BandRequestHandle handle, \n    BandTensor** output_tensors, size_t num_outputs);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBlocks until the request is finished. Returns \u003ccode\u003ekBandOk\u003c/code\u003e if the execution is successful and the result is stored in the output tensors.\u003c/p\u003e\n\u003ch3 id=\"bandenginesetonendrequest\"\u003eBandEngineSetOnEndRequest \u003ca href=\"#bandenginesetonendrequest\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandEngineSetOnEndRequest(\n    BandEngine* engine,\n    void (*on_end_invoke)(void* user_data, BandRequestHandle job_id, BandStatus status),\n    void* user_data);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets a callback function that will be invoked when a request is finished. The callback function will be invoked in the engine thread. The \u003ccode\u003estatus\u003c/code\u003e is the status of the request.\u003c/p\u003e\n\u003ch2 id=\"api-functions-buffer\"\u003eAPI Functions (Buffer) \u003ca href=\"#api-functions-buffer\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides a buffer type that can be used to hold data for a tensor. The buffer can be created from a raw data pointer. The buffer can be converted to a tensor using an image processor.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eImageProcessorBuilder\u003c/code\u003e is used to build an \u003ccode\u003eImageProcessor\u003c/code\u003e. \u003ccode\u003eImageProcessor\u003c/code\u003e defines a series of operations to be applied to a \u003ccode\u003eBandBuffer\u003c/code\u003e and convert it to a \u003ccode\u003eBandTensor\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eBy default, builder without any operation will create a \u003ccode\u003eImageProcessor\u003c/code\u003e\nprovides a direct mapping from \u003ccode\u003eBandBuffer\u003c/code\u003e to \u003ccode\u003eBandTensor\u003c/code\u003e without normalization. E.g., automated color space conversion, resize to the output tensor shape, and data type conversion.\u003c/p\u003e\n\u003ch3 id=\"bandbuffercreate\"\u003eBandBufferCreate \u003ca href=\"#bandbuffercreate\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandBuffer* BandBufferCreate();\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a buffer instance.\u003c/p\u003e\n\u003ch3 id=\"bandbufferdelete\"\u003eBandBufferDelete \u003ca href=\"#bandbufferdelete\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandBufferDelete(BandBuffer* buffer);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDeletes the buffer instance.\u003c/p\u003e\n\u003ch3 id=\"bandbuffersetfromrawdata\"\u003eBandBufferSetFromRawData \u003ca href=\"#bandbuffersetfromrawdata\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandStatus BandBufferSetFromRawData(BandBuffer* buffer, const void* data,\n                                    size_t width, size_t height,\n                                    BandBufferFormat format);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the buffer from raw image data. Supported formats are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekBandRGB\u003c/code\u003e (3 channels - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandRGBA\u003c/code\u003e (4 channels - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandGRAY\u003c/code\u003e (1 channel - 8 bits per channel)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandNV21\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandNV12\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandYV12\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, planar)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandYV21\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, planar)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"bandbuffersetfromyuvdata\"\u003eBandBufferSetFromYUVData \u003ca href=\"#bandbuffersetfromyuvdata\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandStatus BandBufferSetFromYUVData(BandBuffer* buffer, const void* y_data,\n                                    const void* u_data, const void* v_data,\n                                    size_t width, size_t height,\n                                    size_t row_stride_y, size_t row_stride_uv,\n                                    size_t pixel_stride_uv,\n                                    BandBufferFormat buffer_format);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the buffer from YUV data. Supported formats are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ekBandNV21\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandNV12\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, interleaved)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandYV12\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, planar)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ekBandYV21\u003c/code\u003e (YUV 4:2:0 - 8 bits per channel, planar)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"bandimageprocessorbuildercreate\"\u003eBandImageProcessorBuilderCreate \u003ca href=\"#bandimageprocessorbuildercreate\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandImageProcessorBuilder* BandImageProcessorBuilderCreate();\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an image processor builder instance.\u003c/p\u003e\n\u003ch3 id=\"bandimageprocessorbuilderdelete\"\u003eBandImageProcessorBuilderDelete \u003ca href=\"#bandimageprocessorbuilderdelete\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandImageProcessorBuilderDelete(BandImageProcessorBuilder* builder);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDeletes the image processor builder instance.\u003c/p\u003e\n\u003ch3 id=\"bandimageprocessorbuilderbuild\"\u003eBandImageProcessorBuilderBuild \u003ca href=\"#bandimageprocessorbuilderbuild\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandImageProcessor* BandImageProcessorBuilderBuild(\n    BandImageProcessorBuilder* builder);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBuilds an image processor instance from the builder.\u003c/p\u003e\n\u003ch3 id=\"bandaddoperator\"\u003eBandAddOperator \u003ca href=\"#bandaddoperator\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003e\nBandStatus BandAddOperator(BandImageProcessorBuilder* b,\n                           BandImageProcessorBuilderField field, int count, ...);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdds an operator to the builder. The order of the operators will be the order of the operations applied to the input buffer. E.g., \u003ccode\u003eBandAddOperator(builder, BAND_IMAGE_PROCESSOR_CROP, 4, 0, 0, 100, 100);\u003c/code\u003e will crop the input buffer from (0, 0) to (100, 100). This will return \u003ccode\u003ekBandError\u003c/code\u003e if the given variadic arguments are invalid.\nAvailable operators are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_CROP\u003c/code\u003e: Crops the input buffer. \u003ccode\u003eint x0, int y0, int x1, int y1\u003c/code\u003e - crop from top-left corner, inclusive\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_RESIZE\u003c/code\u003e: Resizes the input buffer. \u003ccode\u003eint width, int height\u003c/code\u003e - resize to a new size\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_ROTATE\u003c/code\u003e: Rotates the input buffer. \u003ccode\u003efloat angle\u003c/code\u003e - counter-clockwise, between 0 and 360 in multiples of 90\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_FLIP\u003c/code\u003e: Flips the input buffer. \u003ccode\u003ebool horizontal, bool vertical\u003c/code\u003e - flip horizontally and/or vertically\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_CONVERT_COLOR_SPACE\u003c/code\u003e: Converts the color space of the input buffer. \u003ccode\u003eBandBufferFormat target_format\u003c/code\u003e - convert the color space\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_NORMALIZE\u003c/code\u003e: Normalizes the input buffer. \u003ccode\u003efloat mean, float std\u003c/code\u003e - normalize the input buffer\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eBAND_IMAGE_PROCESSOR_DATA_TYPE_CONVERT\u003c/code\u003e: Converts the data type of the input buffer. No argument required Convert the data type to the output data type. E.g., convert from 8-bit RGB to 32-bit float RGB (tensor).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"bandimageprocessorprocess\"\u003eBandImageProcessorProcess \u003ca href=\"#bandimageprocessorprocess\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003eBandStatus BandImageProcessorProcess(BandImageProcessor* image_processor,\n                                     BandBuffer* buffer,\n                                     BandTensor* target_tensor);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eApplies the image processor to the input buffer and stores the result in the target tensor. Returns \u003ccode\u003ekBandOk\u003c/code\u003e if the operation is successful.\u003c/p\u003e\n\u003ch3 id=\"bandimageprocessordelete\"\u003eBandImageProcessorDelete \u003ca href=\"#bandimageprocessordelete\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-c\"\u003evoid BandImageProcessorDelete(BandImageProcessor* processor);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDeletes the image processor instance.\u003c/p\u003e\n"
      },
    {
        id: 11,
        href: "https://mrsnu.github.io/docs/api/java/",
        title: "Java",
        description: "One page summary of how to use a Java API.",
        content: "\u003ch2 id=\"introduction\"\u003eIntroduction \u003ca href=\"#introduction\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides a Java API to support Android native applications. This document provides a quick overview of how to use the Java API.\u003c/p\u003e\n\u003ch3 id=\"example\"\u003eExample \u003ca href=\"#example\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe following example shows how to use the Java API to create an engine, register a model, create input and output tensors, and run the model. \u003ca href=\"https://github.com/mrsnu/band-example/java\"\u003eLink\u003c/a\u003e provides a complete example of how to use the Java API to run a model on an image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e\nimport java.util.List;\nimport java.util.Arrays;\nimport java.util.ArrayList;\n\nimport org.mrsnu.band.BackendType;\nimport org.mrsnu.band.Band;\nimport org.mrsnu.band.Buffer;\nimport org.mrsnu.band.BufferFormat;\nimport org.mrsnu.band.Config;\nimport org.mrsnu.band.ConfigBuilder;\nimport org.mrsnu.band.CpuMaskFlag;\nimport org.mrsnu.band.Device;\nimport org.mrsnu.band.Engine;\nimport org.mrsnu.band.ImageProcessor;\nimport org.mrsnu.band.ImageProcessorBuilder;\nimport org.mrsnu.band.Model;\nimport org.mrsnu.band.Request;\nimport org.mrsnu.band.SchedulerType;\nimport org.mrsnu.band.SubgraphPreparationType;\nimport org.mrsnu.band.Tensor;\n\nEngine engine;\nImageProcessor processor;\nModel classifier;\n\nList\u0026lt;Tensor\u0026gt; inputs = new ArrayList\u0026lt;\u0026gt;();\nList\u0026lt;Tensor\u0026gt; outputs = new ArrayList\u0026lt;\u0026gt;();\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n  // 1. Load the Band library\n  Band.init();\n\n  // 2. Create a configuration for the engine.\n  b = new ConfigBuilder();\n  b.addPlannerLogPath(\u0026quot;/data/local/tmp/log.json\u0026quot;);\n  b.addSchedulers(\n          new SchedulerType[]{SchedulerType.HETEROGENEOUS_EARLIEST_FINISH_TIME});\n  b.addMinimumSubgraphSize(7);\n  b.addSubgraphPreparationType(SubgraphPreparationType.MERGE_UNIT_SUBGRAPH);\n  b.addCPUMask(CpuMaskFlag.ALL);\n  b.addPlannerCPUMask(CpuMaskFlag.PRIMARY);\n  b.addWorkers(new Device[]{Device.CPU, Device.GPU, Device.DSP, Device.NPU});\n  b.addWorkerNumThreads(new int[]{1, 1, 1, 1});\n  b.addWorkerCPUMasks(new CpuMaskFlag[]{CpuMaskFlag.ALL, CpuMaskFlag.ALL,\n          CpuMaskFlag.ALL, CpuMaskFlag.ALL});\n  b.addSmoothingFactor(0.1f);\n  b.addProfileDataPath(\u0026quot;/data/local/tmp/profile.json\u0026quot;);\n  b.addOnline(true);\n  b.addNumWarmups(1);\n  b.addNumRuns(1);\n  b.addAllowWorkSteal(true);\n  b.addAvailabilityCheckIntervalMs(30000);\n  b.addScheduleWindowSize(10);\n\n  // 3. Create an engine with the configuration.\n  Config config = b.build();\n  engine = new Engine(config);\n\n  // 4. Create a model from a file and register it to the engine.\n  classifier = new Model(BackendType.TFLITE, \u0026quot;/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite\u0026quot;);\n  engine.registerModel(classifier);\n\n  // 5. Create input and output tensors.\n  inputs.add(engine.createInputTensor(classifier, 0));\n  outputs.add(engine.createOutputTensor(classifier, 0));\n\n  // 6. Create an image processor to preprocess the input image.\n  ImageProcessorBuilder processorBuilder = new ImageProcessorBuilder();\n  processorBuilder.addColorSpaceConvert(BufferFormat.RGB);\n  processorBuilder.addResize(224, 224);\n  processorBuilder.addDataTypeConvert();\n  processor = processorBuilder.build();\n}\n\n// called when an image is available (e.g. from camera2 API)\nprivate void processImage(Image image) {\n  // 7. Preprocess the image and run the model.\n  final Buffer buffer = new Buffer(image.getPlanes(), image.getWidth(), image.getHeight(), BufferFormat.YV12);\n  preprocessor.process(buffer, inputs.get(0));\n  // 8. Run the model.\n  engine.requestSync(classifier, inputs, outputs);\n  // 9. Postprocess the output.\n  ByteBuffer rawResults =\n          outputs.get(0).getData().order(ByteOrder.nativeOrder());\n  rawResults.rewind();\n  FloatBuffer results = rawResults.asFloatBuffer();\n  float[] resultArray = new float[results.remaining()];\n  int class_index = 0;\n  float max = 0;\n  for (int i = 0; i \u0026lt; results.remaining(); i++) {\n    float value = results.get(i);\n    if (value \u0026gt; max) {\n      max = value;\n      class_index = i;\n    }\n  }\n  // 10. Use the result (e.g., class_index should be 282 (tiger cat) for cat image)\n}\n\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"java-api-classes\"\u003eJava API classes \u003ca href=\"#java-api-classes\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe Java API is composed of the following core classes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConfig\u003c/li\u003e\n\u003cli\u003eConfigBuilder\u003c/li\u003e\n\u003cli\u003eEngine\u003c/li\u003e\n\u003cli\u003eModel\u003c/li\u003e\n\u003cli\u003eTensor\u003c/li\u003e\n\u003cli\u003eQuantization\u003c/li\u003e\n\u003cli\u003eRequest\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOptional classes for preprocessing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuffer\u003c/li\u003e\n\u003cli\u003eImageProcessor\u003c/li\u003e\n\u003cli\u003eImageProcessorBuilder\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEnums:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBackendType\u003c/li\u003e\n\u003cli\u003eBufferFormat\u003c/li\u003e\n\u003cli\u003eCpuMaskFlag\u003c/li\u003e\n\u003cli\u003eDataType\u003c/li\u003e\n\u003cli\u003eLogSeverity\u003c/li\u003e\n\u003cli\u003eSchedulerType\u003c/li\u003e\n\u003cli\u003eSubgraphPreparationType\u003c/li\u003e\n\u003cli\u003eTensorType\u003c/li\u003e\n\u003cli\u003eWorkerType\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe will cover Engine, Model, Tensor, and Buffer classes in detail in the following sections.\u003c/p\u003e\n\u003ch2 id=\"api-functions-engine\"\u003eAPI Functions (Engine) \u003ca href=\"#api-functions-engine\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"engineconfig-config\"\u003eEngine(Config config) \u003ca href=\"#engineconfig-config\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Engine(Config config)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an engine with the given configuration.\u003c/p\u003e\n\u003ch3 id=\"registermodel\"\u003eregisterModel \u003ca href=\"#registermodel\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void registerModel(Model model)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRegisters a model to the engine. The model must be registered before it can be used to run inference.\u003c/p\u003e\n\u003ch3 id=\"getnuminputtensors\"\u003egetNumInputTensors \u003ca href=\"#getnuminputtensors\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic int getNumInputTensors(Model model)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of input tensors of the given model.\u003c/p\u003e\n\u003ch3 id=\"getnumoutputtensors\"\u003egetNumOutputTensors \u003ca href=\"#getnumoutputtensors\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic int getNumOutputTensors(Model model)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of output tensors of the given model.\u003c/p\u003e\n\u003ch3 id=\"requestsync\"\u003erequestSync \u003ca href=\"#requestsync\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void requestSync(Model model, List\u0026lt;Tensor\u0026gt; inputTensors, List\u0026lt;Tensor\u0026gt; outputTensors)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuns the given model synchronously with the given input and output tensors. The input and output tensors must be created by the engine.\u003c/p\u003e\n\u003ch3 id=\"requestasync\"\u003erequestAsync \u003ca href=\"#requestasync\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Request requestAsync(Model model, List\u0026lt;Tensor\u0026gt; inputTensors)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuns the given model asynchronously with the given input tensors. The input tensors must be created by the engine. Returns a request object that can be used to wait for the result.\u003c/p\u003e\n\u003ch3 id=\"requestasyncbatch\"\u003erequestAsyncBatch \u003ca href=\"#requestasyncbatch\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic List\u0026lt;Request\u0026gt; requestAsyncBatch(List\u0026lt;Model\u0026gt; models, List\u0026lt;List\u0026lt;Tensor\u0026gt;\u0026gt; inputTensorLists)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRuns the given models asynchronously with the given input tensors. The input tensors must be created by the engine. Returns a list of request objects that can be used to wait for the results.\u003c/p\u003e\n\u003ch3 id=\"wait\"\u003ewait \u003ca href=\"#wait\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void wait(Request request, List\u0026lt;Tensor\u0026gt; outputTensors)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWaits for the given request to finish and copies the output tensors to the given output tensors. The output tensors must be created by the engine.\u003c/p\u003e\n\u003ch3 id=\"createinputtensor\"\u003ecreateInputTensor \u003ca href=\"#createinputtensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Tensor createInputTensor(Model model, int index)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an input tensor for the given model and index. The input tensor can be used to run the model.\u003c/p\u003e\n\u003ch3 id=\"createoutputtensor\"\u003ecreateOutputTensor \u003ca href=\"#createoutputtensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e\npublic Tensor createOutputTensor(Model model, int index)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates an output tensor for the given model and index. The output tensor can be used to run the model.\u003c/p\u003e\n\u003ch2 id=\"api-functions-model\"\u003eAPI Functions (Model) \u003ca href=\"#api-functions-model\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"modelbackendtype-backendtype-string-filepath\"\u003eModel(BackendType backendType, String filePath) \u003ca href=\"#modelbackendtype-backendtype-string-filepath\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Model(BackendType backendType, String filePath)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a model from the given file path. The model should be registered to the engine before it can be used to run inference.\u003c/p\u003e\n\u003ch3 id=\"modelbackendtype-backendtype-bytebuffer-modelbuffer\"\u003eModel(BackendType backendType, ByteBuffer modelBuffer) \u003ca href=\"#modelbackendtype-backendtype-bytebuffer-modelbuffer\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Model(BackendType backendType, ByteBuffer modelBuffer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a model from the given byte buffer. The model should be registered to the engine before it can be used to run inference.\u003c/p\u003e\n\u003ch3 id=\"getsupportedbackends\"\u003egetSupportedBackends \u003ca href=\"#getsupportedbackends\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic List\u0026lt;BackendType\u0026gt; getSupportedBackends()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns a list of supported backends for the model.\u003c/p\u003e\n\u003ch2 id=\"api-functions-tensor\"\u003eAPI Functions (Tensor) \u003ca href=\"#api-functions-tensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"gettype\"\u003egetType \u003ca href=\"#gettype\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic DataType getType()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the data type of the tensor.\u003c/p\u003e\n\u003ch3 id=\"settype\"\u003esetType \u003ca href=\"#settype\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setType(DataType dataType)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the data type of the tensor.\u003c/p\u003e\n\u003ch3 id=\"getdata\"\u003egetData \u003ca href=\"#getdata\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic ByteBuffer getData()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the data buffer of the tensor.\u003c/p\u003e\n\u003ch3 id=\"setdata\"\u003esetData \u003ca href=\"#setdata\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setData(ByteBuffer data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the data buffer of the tensor.\u003c/p\u003e\n\u003ch3 id=\"getdims\"\u003egetDims \u003ca href=\"#getdims\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic int[] getDims()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the dimensions of the tensor.\u003c/p\u003e\n\u003ch3 id=\"setdims\"\u003esetDims \u003ca href=\"#setdims\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setDims(int[] dims)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the dimensions of the tensor.\u003c/p\u003e\n\u003ch3 id=\"getbytes\"\u003egetBytes \u003ca href=\"#getbytes\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003e\npublic int getBytes()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the number of bytes of the tensor.\u003c/p\u003e\n\u003ch3 id=\"getname\"\u003egetName \u003ca href=\"#getname\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic String getName()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the name of the tensor.\u003c/p\u003e\n\u003ch3 id=\"getquantization\"\u003egetQuantization \u003ca href=\"#getquantization\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Quantization getQuantization()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReturns the quantization of the tensor.\u003c/p\u003e\n\u003ch3 id=\"setquantization\"\u003esetQuantization \u003ca href=\"#setquantization\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic void setQuantization(Quantization quantization)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSets the quantization of the tensor.\u003c/p\u003e\n\u003ch2 id=\"api-functions-buffer\"\u003eAPI Functions (Buffer) \u003ca href=\"#api-functions-buffer\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBuffer is a helper class to wrap multiple types of memory buffers (e.g., byte buffer, image planes) into a single class. It is used to preprocess input images before running inference. It does not deep copy the data, so the data must be outlived the buffer.\u003c/p\u003e\n\u003ch3 id=\"buffertensor-tensor\"\u003eBuffer(Tensor tensor) \u003ca href=\"#buffertensor-tensor\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Buffer(Tensor tensor)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a buffer from the given tensor. The tensor must be created by the engine.\u003c/p\u003e\n\u003ch3 id=\"bufferbyte-buffer-int-width-int-height-bufferformat-bufferformat\"\u003eBuffer(byte[] buffer, int width, int height, BufferFormat bufferFormat) \u003ca href=\"#bufferbyte-buffer-int-width-int-height-bufferformat-bufferformat\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Buffer(byte[] buffer, int width, int height, BufferFormat bufferFormat)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a buffer from the given byte buffer, width, height, and format. We currently support RGB, RGBA, and GRAYSCALE formats.\u003c/p\u003e\n\u003ch3 id=\"bufferbyte-yuvbytes-int-width-int-height-int-yrowstride-int-uvrowstride-int-uvpixelstride-bufferformat-bufferformat\"\u003eBuffer(byte[][] yuvBytes, int width, int height, int yRowStride, int uvRowStride, int uvPixelStride, BufferFormat bufferFormat) \u003ca href=\"#bufferbyte-yuvbytes-int-width-int-height-int-yrowstride-int-uvrowstride-int-uvpixelstride-bufferformat-bufferformat\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Buffer(byte[][] yuvBytes, int width, int height, int yRowStride, int uvRowStride, int uvPixelStride, BufferFormat bufferFormat)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a buffer from the given YUV byte buffer, width, height, row strides, and format. We currently support YUV formats (NV21, NV12, YV12, YV21).\u003c/p\u003e\n\u003ch3 id=\"bufferimageplane-planes-int-width-int-height-bufferformat-format\"\u003eBuffer(Image.Plane[] planes, int width, int height, BufferFormat format) \u003ca href=\"#bufferimageplane-planes-int-width-int-height-bufferformat-format\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-java\"\u003epublic Buffer(Image.Plane[] planes, int width, int height, BufferFormat format)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreates a buffer from the given planes, width, height, and format. We currently support YUV formats (NV21, NV12, YV12, YV21).\u003c/p\u003e\n"
      },
    {
        id: 12,
        href: "https://mrsnu.github.io/docs/api/unreal-plugin/",
        title: "Unreal Engine Plugin",
        description: "One page summary of how to use the Unreal Engine Plugin.",
        content: "\u003ch2 id=\"introduction\"\u003eIntroduction \u003ca href=\"#introduction\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBand provides an official \u003ca href=\"https://github.com/mrsnu/band-ue\"\u003eplugin\u003c/a\u003e for Unreal Engine.\nWe support both \u003ccode\u003eBlueprint\u003c/code\u003e (visual scripting language of the UE) and C++ based interfaces.\u003c/p\u003e\n\u003ch2 id=\"example\"\u003eExample \u003ca href=\"#example\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe first demonstrate how to use the plugin with a simple example.\nBelow code snippet is part of the \u003ccode\u003emobile augmented-reality classification\u003c/code\u003e example built with the C++ interface of the plugin.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/example/\"\u003eLink to Example\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"c\"\u003eC++ \u003ca href=\"#c\" class=\"anchor\" aria-hidden=\"true\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe main workflow of the example is as follows:\nInitialize the plugin and load the model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePreallocate the input and output tensors.\u003c/li\u003e\n\u003cli\u003eCreate a widget to display the camera image and the classification result.\u003c/li\u003e\n\u003cli\u003eStart the camera (we use our own camera \u003ca href=\"https://github.com/snuhcs/android-camera-ue\"\u003eplugin\u003c/a\u003e to access the Android camera).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003e// Called when the game starts or when spawned\nvoid ADetectionActor::BeginPlay() {\n  DetectorInputTensors = DetectorModel-\u0026gt;AllocateInputTensors();\n  DetectorOutputTensors = DetectorModel-\u0026gt;AllocateOutputTensors();\n\n  TSubclassOf\u0026lt;UBandUIBase\u0026gt; WidgetClassType = WidgetClass.LoadSynchronous();\n  Widget = CreateWidget\u0026lt;UBandUIBase\u0026gt;(GetWorld(), WidgetClassType);\n  Widget-\u0026gt;AddToViewport();\n\n  CameraImage = Cast\u0026lt;UImage\u0026gt;(Widget-\u0026gt;GetWidgetFromName(\u0026quot;CameraImage\u0026quot;));\n  GetGameInstance()-\u0026gt;GetSubsystem\u0026lt;UBandSubSystem\u0026gt;()-\u0026gt;OnEndInvoke.AddUObject(\n      this, \u0026amp;ADetectionActor::OnEndRequest);\n\n  AndroidCamera-\u0026gt;StartCamera(640, 640, 30);\n  AndroidCamera-\u0026gt;OnFrameAvailable.AddUObject(\n      this, \u0026amp;ADetectionActor::OnFrameAvailable);\n  AndroidCamera-\u0026gt;OnTextureAvailableDynamic.AddDynamic(\n      this, \u0026amp;ADetectionActor::OnTextureAvailable);\n\n  Super::BeginPlay();\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen a new frame is available, we feed the frame to the model and request the inference.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003evoid ADetectionActor::OnFrameAvailable(const UAndroidCameraFrame *Frame) {\n  DetectorInputTensors[0]-\u0026gt;FromCameraFrame(Frame, 0.f, 1.f);\n  FBandModule::Get().RequestAsync(DetectorModel, DetectorInputTensors);\n}\n\n\nvoid ADetectionActor::OnTextureAvailable(UTexture2D *Texture) {\n  CameraImage-\u0026gt;SetBrushFromTexture(Texture, true);\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen the inference is done, we get the result and update the widget.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-cpp\"\u003evoid ADetectionActor::OnEndRequest(int32 JobId, EBandStatus Status) {\n  if (Status == EBandStatus::Ok) {\n    UBandBlueprintLibrary::GetOutputs(JobId, DetectorOutputTensors);\n    TArray\u0026lt;FBandBoundingBox\u0026gt; BoundingBoxes =\n        UBandBlueprintLibrary::GetDetectedBoxes(\n            DetectorOutputTensors, EBandDetector::SSDMNetV2, Label);\n    Widget-\u0026gt;BoundingBoxes = BoundingBoxes;\n  } else {\n    UE_LOG(LogTemp, Error, TEXT(\u0026quot;Detection failed\u0026quot;));\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
    {
        id: 13,
        href: "https://mrsnu.github.io/docs/",
        title: "Docs",
        description: "Docs Doks.",
        content: ""
      },
    ];
  */

  // https://discourse.gohugo.io/t/range-length-or-last-element/3803/2

  

  index.add(
      {
        id: 0,
        href: "/docs/prologue/",
        title: "Prologue",
        description: "Prologue Doks.",
        content: ""
      }
    );
  index.add(
      {
        id: 1,
        href: "/docs/getting-started/",
        title: "Getting Started",
        description: "Getting Started",
        content: ""
      }
    );
  index.add(
      {
        id: 2,
        href: "/docs/api/",
        title: "API",
        description: "API",
        content: ""
      }
    );
  index.add(
      {
        id: 3,
        href: "/docs/prologue/introduction/",
        title: "Introduction",
        description: "Band is an efficient deep learning platform for mobile-cloud collaborative support for multiple DNNs. Band supports backend-agnostic coordination of DNN requests on heterogeneous processors in a mobile device to cloud server. Band is currently backed by following backend machine learning frameworks.\nTensorflow v2.9.2 \u0026hellip; Android â˜‘ iOS â˜ gRPC â˜ Band provides Java and C APIs, as well as an official plugin for Unreal Engine.\nQuick Start # ðŸ‘‰ The Quick Start is intended for intermediate users.",
        content: "Band is an efficient deep learning platform for mobile-cloud collaborative support for multiple DNNs. Band supports backend-agnostic coordination of DNN requests on heterogeneous processors in a mobile device to cloud server. Band is currently backed by following backend machine learning frameworks.\nTensorflow v2.9.2 \u0026hellip; Android â˜‘ iOS â˜ gRPC â˜ Band provides Java and C APIs, as well as an official plugin for Unreal Engine.\nQuick Start # ðŸ‘‰ The Quick Start is intended for intermediate users. One page summary of how to build Android Band application. Quick Start â†’\nExamples # See what others have build with Band. Example â†’\nHow to build and modify Band # ðŸ‘‰ The Quick Start is intended for advanced users. One page summary of how to build and customize Band. Build Guide â†’\nAbout Us # Find out who we are. About Us-\u0026gt;\n"
      }
    );
  index.add(
      {
        id: 4,
        href: "/docs/getting-started/quick-start/",
        title: "Quick Start",
        description: "One page summary of how to use a Band.",
        content: "In this guide, we will show you how to use Band to develop Android application.\nSteps to use Band # Install Android Studio\nClone Android example project and open band-example/android/object_detection it with Android Studio\ngit clone https://github.com/mrsnu/band-example.git Change the application ID in band-example/android/object_detection/app/build.gradle to your own application ID\ndefaultConfig { applicationId \u0026quot;com.band.example\u0026quot; } Update the package name in band-example/android/object_detection/app/src/main/AndroidManifest.xml to your own package name\n\u0026lt;manifest xmlns:android=\u0026quot;http://schemas.android.com/apk/res/android\u0026quot; package=\u0026quot;com.band.example\u0026quot;\u0026gt; Change the application name in band-example/android/object_detection/app/src/main/res/values/strings.xml to your own application name\n\u0026lt;resources\u0026gt; \u0026lt;string name=\u0026quot;app_name\u0026quot;\u0026gt;Band Example\u0026lt;/string\u0026gt; \u0026lt;/resources\u0026gt; Start developing your own application!\n"
      }
    );
  index.add(
      {
        id: 5,
        href: "/docs/prologue/about-us/",
        title: "About Us",
        description: "Band is built and designed by the team `SNUMR` from Seoul National University Dept. of Computer Science and Engineering. We are cross-laboratory team from Software Platforms Lab and Human-Centered Computer-Systems Lab.",
        content: "Current Members # Youngki Lee, Byung-Gon Chun\nJingyu Lee, Changmin Jeon, Minjae Kim, Hyunsoo Kim, Seonjun Kim\nPrevious Members # Joo Seong Jeong, Donghyun Kim, Changjin Jeong\nCitation # If you find our work useful, please cite our paper below! The original codebase for paper submission is archived here\n@inproceedings{jeong2022band, title={Band: coordinated multi-DNN inference on heterogeneous mobile processors}, author={Jeong, Joo Seong and Lee, Jingyu and Kim, Donghyun and Jeon, Changmin and Jeong, Changjin and Lee, Youngki and Chun, Byung-Gon}, booktitle={Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services}, pages={235--247}, year={2022} } Acknowledgment # This work was supported by Samsung Research Funding \u0026amp; Incubation Center of Samsung Electronics under project number SRFC-IT2001-03.\n"
      }
    );
  index.add(
      {
        id: 6,
        href: "/docs/getting-started/benchmark/",
        title: "Benchmark Tool",
        description: "One page summary of how to use a Benchmark Tool for Band.",
        content: "Band provides a simple C++ binary to benchmark a runtime performance. The binary generates repeatitive model requests based on a given config file, and reports latency statistics afterwards.\nHow to run # [root]/script/run_benchmark.py script will build band_benchmark binary file and execute it with a specified config file. Built binary file and target config file can be found in [root]/benchmark.\nOn Android # If you want to build binary from docker container (Refer to [root]/script/docker_util.sh for more detail)\npython .\\script\\run_benchmark.py -android -docker -c .\\benchmark_config.json If you want to build locally\npython .\\script\\run_benchmark.py -android -c .\\benchmark_config.json On local desktop (Windows or Ubuntu) # python .\\script\\run_benchmark.py -c .\\benchmark_config.json Config file # Structure # models: Models to run. For each model, specify the following fields. graph: Model path. period_ms: Optional The delay between subsequent requests in ms. The argument is only effective with periodic execution mode. batch_size: The number of model requests in a frame. [default: 1] worker_id: Optional Specify the worker id to run in int. The argument is only effective with fixed_device scheduler. slo_us and slo_scale: Optional fields for specifying an SLO value for a model. Setting slo_scale will make the SLO = worst profiled latency of that model * slo_scale. slo_scale will be ignored if slo_us is given (i.e., no reason to specify both options). log_path: The log file path. (e.g., /data/local/tmp/model_execution_log.json) schedulers: The scheduler types in list[string]. If N schedulers are specified, then N queues are generated. fixed_worker round_robin shortest_expected_latency least_slack_time_first heterogeneous_earliest_finish_time heterogeneous_earliest_finish_time_reserved minimum_subgraph_size: Minimum subgraph size. If candidate subgraph size is smaller than minimum_subgraph_size, the subgraph will not be created. [default: 7] subgraph_preparation_type: For schedulers using fallback, determine how to generate candidate subgraphs. [default: merge_unit_subgraph] no_fallback_subgraph: Generate subgraphs per worker. Explicit fallback subgraph will not be generated. fallback_per_worker: Generate fallback subgraphs for each worker. unit_subgraph: Generate unit subgraphs considering all device supportiveness. All ops in same unit subgraph have same support devices. merge_unit_subgraph: Add merged unit subgraphs to unit_subgraph. execution_mode: Specify a exeucution mode. Available execution modes are as follows: stream: consecutively run batches. periodic: invoke requests periodically. workload: execute pre-defined sequence in stream manner based on a given workload file. cpu_masks: CPU cluster mask to set CPU affinity. [default: ALL] ALL: All Cluster LITTLE: LITTLE Cluster only BIG: Big Cluster only PRIMARY: Primary Core only num_threads: Number of computing threads for CPU delegates. [default: -1] planner_cpu_masks: CPU cluster mask to set CPU affinity of planner. [default: same value as global cpu_masks] workers: A vector-like config for per-processor worker. For each worker, specify the following fields. System creates 1 worker per device by default and first provided value overrides the settings (i.e., cpu_masks, num_threads, profile_copy_computation_ratio, \u0026hellip; ) and additional field will add additional worker per device. device: Target device of specific worker. CPU GPU DSP NPU cpu_masks: CPU cluster mask to set CPU affinity of specific worker. [default: same value as global cpu_masks] num_threads: Number of threads. [default: same value as global num_threads] running_time_ms: Experiment duration in ms. [default: 60000] profile_smoothing_factor: Current profile reflection ratio. updated_profile = profile_smoothing_factor * curr_profile + (1 - profile_smoothing_factor) * prev_profile [default: 0.1] model_profile: The path to file with model profile results. [default: None] profile_online: Online profile or offline profile [default: true] profile_warmup_runs: Number of warmup runs before profile. [default: 1] profile_num_runs: Number of runs for profile. [default: 1] profile_copy_computation_ratio: Ratio of computation / input-ouput copy in list[int]. Used for latency estimation for each device type (e.g., CPU, GPU, DSP, NPU). The length of the list should be equal to the 4 (GetSize\u0026lt;DeviceFlags\u0026gt;()). [default: 30000, 30000, 30000, 30000] schedule_window_size: The number of planning unit. workload: The path to file with workload information. [default: None] Example # { \u0026quot;models\u0026quot;: [ { \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/lite-model_efficientdet_lite0_int8_1.tflite\u0026quot;, \u0026quot;period_ms\u0026quot;: 30, \u0026quot;batch_size\u0026quot;: 3 }, { \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/retinaface_mbv2_quant_160.tflite\u0026quot;, \u0026quot;period_ms\u0026quot;: 30, \u0026quot;batch_size\u0026quot;: 3 }, { \u0026quot;graph\u0026quot;: \u0026quot;/data/local/tmp/model/ssd_mobilenet_v1_1_metadata_1.tflite\u0026quot;, \u0026quot;period_ms\u0026quot;: 30, \u0026quot;batch_size\u0026quot;: 3 } ], \u0026quot;log_path\u0026quot;: \u0026quot;/data/local/tmp/log.json\u0026quot;, \u0026quot;schedulers\u0026quot;: [ \u0026quot;heterogeneous_earliest_finish_time_reserved\u0026quot; ], \u0026quot;minimum_subgraph_size\u0026quot;: 7, \u0026quot;subgraph_preparation_type\u0026quot;: \u0026quot;merge_unit_subgraph\u0026quot;, \u0026quot;execution_mode\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;cpu_masks\u0026quot;: \u0026quot;ALL\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;planner_cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot;, \u0026quot;workers\u0026quot;: [ { \u0026quot;device\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;cpu_masks\u0026quot;: \u0026quot;BIG\u0026quot; }, { \u0026quot;device\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;cpu_masks\u0026quot;: \u0026quot;LITTLE\u0026quot; }, { \u0026quot;device\u0026quot;: \u0026quot;GPU\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;cpu_masks\u0026quot;: \u0026quot;ALL\u0026quot; }, { \u0026quot;device\u0026quot;: \u0026quot;DSP\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot; }, { \u0026quot;device\u0026quot;: \u0026quot;NPU\u0026quot;, \u0026quot;num_threads\u0026quot;: 1, \u0026quot;cpu_masks\u0026quot;: \u0026quot;PRIMARY\u0026quot; } ], \u0026quot;running_time_ms\u0026quot;: 10000, \u0026quot;profile_smoothing_factor\u0026quot;: 0.1, \u0026quot;profile_data_path\u0026quot;: \u0026quot;/data/local/tmp/profile.json\u0026quot;, \u0026quot;profile_online\u0026quot;: true, \u0026quot;profile_warmup_runs\u0026quot;: 3, \u0026quot;profile_num_runs\u0026quot;: 50, \u0026quot;profile_copy_computation_ratio\u0026quot;: [ 1000, 1000, 1000, 1000 ], \u0026quot;availability_check_interval_ms\u0026quot;: 30000, \u0026quot;schedule_window_size\u0026quot;: 10 } "
      }
    );
  index.add(
      {
        id: 7,
        href: "/docs/getting-started/config/",
        title: "Config",
        description: "Configuration and its Builder for Band Runtime.",
        content: "There are four configuration objects for each Band\u0026rsquo;s component.\nEach configuration field is optional or required. If a field is optional, then it is guaranteed that the default value exists. If a field is required, a configuration cannot be generated by RuntimeConfigBuilder without specifying the field.\nEnumeration Types # BandSchedulerType\nkFixedDeviceFixedWorker kRoundRobin kShortestExpectedLatency kFixedDeviceGlobalQueue kHeterogeneousEarliestFinishTime kLeastSlackTimeFirst kHeterogeneousEarliestFinishTimeReserved CPUMaskFlags\nkAll kLittle kBig kPrimary DeviceFlags\nkCPU kGPU kDSP kNPU SubgraphPreparationType\nkNoFallbackSubgraph kFallbackPerWorker kUnitSubgraph kMergeUnitSubgraph ProfileConfig # online [type: bool, default: true]: Profile online if true, offline if false. num_warmups [type: int, default: 1]: The number of warmup runs before profile. num_runs [type: int, default: 1]: The number of runs for profile copy_computation_ratio [type: std::vector\u0026lt;int\u0026gt;, default: [30000, ...]]: The ratio of computation to input-output copy. Used for latency estimation. The size of the list should be the same as the number of devices. smoothing_factor [type: float, default: 0.1]: The momentum to reflect current profiled data. \u0026lt;updateed_profile\u0026gt; = \u0026lt;smoothing_factor\u0026gt; * \u0026lt;curr_profile\u0026gt; + (1. - \u0026lt;smoothing_factor\u0026gt;) * \u0026lt;prev_profile\u0026gt;. profile_data_path [type: std::string, default: \u0026quot;\u0026quot;]: The input path to the file for offline profile results. If not specified, this will be ignored and will not generate the result file. PlannerConfig # schedule_window_size [type: int, default: INT_MAX]: The size of window that scheduler will use. schedulers [type: std::vector\u0026lt;SchedulerType\u0026gt;, required]: The types of schedulers. If N schedulers are specified, N queues will be generated. cpu_mask [type: CPUMaskFlags, default: kAll]: CPU masks to set CPU affinity. log_path [type: std::string, default: \u0026quot;\u0026quot;]: The output path to the file for planner\u0026rsquo;s log. If not specified, this will be ignored and will not generate the result file. WorkerConfig # workers [type: std::vector\u0026lt;DeviceFlags\u0026gt;, default: [kCPU, kGPU, ...]]: The list of target devices. By default, one worker per device is generated. cpu_masks [type: std::vector\u0026lt;CPUMaskFlags\u0026gt;, default: [kAll, kAll, ...]]: CPU masks to set CPU affinity. The size of the list must be the same as the size of workers. num_threads [type: std::vector\u0026lt;int\u0026gt;, default: [1, 1, ...]]: The number of threads. The size of the list must be the same as the size of workers. allow_worksteal [type: bool, default: false]: Work-stealing is enabled if true, disabled if false. availability_check_interval_ms [type: int, default: 30_000]: The interval for checking availability of devices. Used for detecting thermal throttling. RuntimeConfig # RuntimeConfig contains ProfileConfig, PlannerConfig and WorkerConfig. minimum_subgraph_size [type: int, default: 7]: The minimum subgraph size. If candidate subgraph size is smaller than this, the subgraph will not be created. subgraph_preparation_type [type: SubgraphPreparationType, default: kMergeUnitSubgraph]: For fallback schedulers, determine how to generate candidate subgraphs. cpu_mask [type: CPUMaskFlags, default: kAll]: The CPU mask for Band Engine. RuntimeConfigBuilder API # RuntimeConfigBuilder delegates all builder that inherits ConfigBuilder. It is a friend class of all the other ConfigBuilder classes, so make sure to not change their members in RuntimeConfigBuilder.\nExmaple Usage # RuntimeConfigBuilder b; auto config = b.AddOnline(false) // Default was `true` .AddSmoothingFactor(0.3) // Default was `0.1` .AddSchedulers({SchedulerType::kRoundRobin, SchedulerType::kLeastSlackTimeFirst}) // Required field. .Build(); Methods # All Add* methods are idempotent, i.e. multiple calls behaves the same as a single call.\nAddOnline(bool online) AddNumWarmups(int num_warmups) AddNumRuns(int num_runs) AddCopyComputationRatio(std::vector\u0026lt;int\u0026gt; copy_computation_ratio) AddSmoothingFactor(float smoothing_factor) AddProfileLogPath(std::string profile_data_path) AddPlannerLogPath(std::string planner_log_path) AddScheduleWindowSize(int schedule_window_size) AddSchedulers(std::vector\u0026lt;SchedulerType\u0026gt; schedulers) AddPlannerCPUMask(CPUMaskFlags cpu_masks) AddWorkers(std::vector\u0026lt;DeviceFlags\u0026gt; workers) AddWorkerCPUMasks(std::vector\u0026lt;CPUMaskFlags\u0026gt; cpu_masks) AddWorkerNumThreads(std::vector\u0026lt;int\u0026gt; num_threads) AddAllowWorkSteal(bool allow_worksteal) AddAvailabilityCheckIntervalMs(int32_t availability_check_interval_ms) AddMinimumSubgraphSize(int minimum_subgraph_size) AddSubgraphPreparationType(SubgraphPreparationType subgraph_preparation_type) AddCPUMask(CPUMaskFlags cpu_mask) "
      }
    );
  index.add(
      {
        id: 8,
        href: "/docs/getting-started/preprocessing/",
        title: "Preprocessing",
        description: "Buffer and its Builder for Band Runtime.",
        content: "Overview # Band provides a set of APIs to preprocess the data. The preprocessing is mendatory and time-consuming process to run the machine learning model on the Band. Band provides Buffer and BufferProcessor to efficiently develop the preprocessing pipeline.\nBuffer is an arbitrary wrapper of the data. It wraps the data with the metadata such as image, text, and Band Tensor to be used in the preprocessing pipeline. BufferProcessor is a set of APIs to preprocess the data. It provides the basic preprocessing APIs such as resize, normalize, and crop. We currently support image preprocessing only but we will support other data types such as text and audio in the future.\nExample Usage - ImageProcessor # Below example shows how to use BufferProcessor to preprocess the image data. The example creates a Buffer from raw RGB data with (width, height) dimentions and (3) channels. Then, it creates an ImageProcessor with Resize and Normalize operations. It preprocesses the Buffer and updates a Tensor with (224, 224) dimentions and (3) channels with kFloat32 data type with the data normalized with (127.5, 127.5).\nTensor* tensor = ... // Create a tensor // Create a buffer unsigned char* data = new unsigned char[width * height * 3]; data = ... // Fill the data Buffer* buffer = Buffer::CreateFromRaw(data, width, height, 3, BufferFormat::kRGB, DataType::kUInt8); // Create an image processor ImageProcessorBuilder builder; builder.SetResize(224, 224); builder.SetNormalize(127.5f, 127.5f); absl::StatusOr\u0026lt;std::unique_ptr\u0026lt;BufferProcessor\u0026gt;\u0026gt; preprocessor = preprocessor_builder.Build(); // Preprocess the buffer preprocessor-\u0026gt;process(*buffer, *tensor); ... // Use the tensor Buffer # Buffer can be created from following data types and metadata:\nraw data, width, height, BufferFormat, DataType, and BufferOrientation (BufferFormat::kGrayScale, BufferFormat::kRGB, BufferFormat::kRGBA, and BufferFormat::kRaw only) y plane, u plane, v plane, width, height, raw stride of y plane, raw stride of uv plane, pixel stride of uv plane, BufferFormat, DataType, and BufferOrientation (BufferFormat::kYV12, BufferFormat::kYV21, BufferFormat::kNV21, and BufferFormat::kNV12 only) Tensor Currently, BufferFormats that are not kRaw only support kUInt8 DataType.\nEnumeration Types # BufferFormat\nkGrayScale - 8-bit gray scale kRGB - 8-bit RGB kRGBA - 8-bit RGBA kNV21 - YUV 4:2:0, 8 bit per channel, interleaved kNV12 - YUV 4:2:0, 8 bit per channel, interleaved kYV12 - YUV 4:2:0, 8 bit per channel, planar kYV21 - YUV 4:2:0, 8 bit per channel, planar kRaw - raw data DataType\nkNoType kFloat32 kInt32 kUInt8 kInt64 kString kBool kInt16 kComplex64 kInt8 kFloat16 kFloat64 BufferProcessor # ImageProcessor # ImageProcessor supports following operations:\nCrop(int x0, int y0, int x1, int y1): crop from top-left corner, inclusive Resize(int width, int height): resize to a new size Rotate(float angle): counter-clockwise, between 0 and 360 in multiples of 90 Flip(bool horizontal, bool vertical) ConvertColorSpace(BufferFormat target_format): convert the color space Normalize(float mean, float std) DataTypeConvert(): convert the data type to the output data type, e.g., convert from 8-bit RGB to 32-bit float RGB (tensor). ImageProcessorBuilder provides a simple way to create an ImageProcessor. The user predefines the operations and ImageProcessorBuilder will create an ImageProcessor with the operations.\nBy default, ImageProcessorBuilder without any operation will create a ImageProcessor provides a direct mapping from entire Buffer to Tensor without normalization. This covers the most common use case of the preprocessing.\n"
      }
    );
  index.add(
      {
        id: 9,
        href: "/docs/getting-started/build-guide/",
        title: "Build Guide (Advanced Users)",
        description: "One page summary of how to build a Band.",
        content: "We provide pre-built binaries for Android and Windows. However, if you want to build Band from source code, please follow the instructions below. It is currently tested on Ubuntu 18.04 and Windows 10.\nPrerequisites # We recommend using Visual Studio Code dev container to build Band. We provide the container to build Band without installing any dependencies. For more details, please refer to Visual Studio Code Dev Container. If you use the container, you can skip to Prerequisites for Android and How to build \u0026amp; run.\nClone Band repository\ngit clone https://github.com/mrsnu/band.git Install submodules\ncd band git submodule update --init --recursive Install Bazel\nInstall Bazelisk or Bazel We recommend to use Bazelisk to avoid version mismatch Prerequisites for Android # Configure Android SDK, NDK\npython configure.py How to build \u0026amp; run # We provide useful scripts to build and run Band. Please refer to [root]/script for more details. Test and Benchmark script allows you to cross-compile from the docker container (from our dev container) on the host machine\u0026rsquo;s docker context and download the built binary to the host machine (-docker option). If you want to build locally, please remove -docker option.\nRun test - Our test script will build and run tests for all platforms (Android, Linux, Windows). If not specified, it will build and run tests for the native platform (Linux or Windows) that executes the script.\npython script/run_test.py -android -docker Build Android AAR - Our build script will build AAR for Android and copy it to [root].\nsh script/build_aar_armv8.sh Build C API - Our build script will build C API for Android, and output files are located at [root]/bazel-bin/band/c/band_c_pkg.tar\npython script/build_c_api.py -android Run Benchmark - Our benchmark script will build and run the benchmark for all platforms (Android, Linux, Windows). If not specified, it will build and run the benchmark for native platforms (Linux or Windows) that execute the script.\npython script/run_benchmark.py --config band/test/data/benchmark_config.json -android -docker "
      }
    );
  index.add(
      {
        id: 10,
        href: "/docs/api/c/",
        title: "C",
        description: "One page summary of how to use a C API.",
        content: "Introduction # Band provides a C API for developers who want to use Band in their C/C++ projects. The API is a thin wrapper around the core C++ API. The C API is available in the libband.h header file.\nLink provides a complete example of how to dynamically load the library and use the C API.\nExample # #include \u0026lt;libband.h\u0026gt; int main() { // 1. Create a configuration for the engine. BandConfigBuilder* b = BandConfigBuilderCreate(); BandAddConfig(b, BAND_PLANNER_LOG_PATH, /*count=*/1, \u0026quot;log.json\u0026quot;); BandAddConfig(b, BAND_PLANNER_SCHEDULERS, /*count=*/1, kBandHeterogeneousEarliestFinishTime); BandConfig* config = BandConfigCreate(b); // 2. Create an engine. BandEngine* engine = BandEngineCreate(config); // 3. Create and register a model. BandModel* model = BandModelCreate(); BandModelAddFromFile(model, kBandTfLite, \u0026quot;mobilenet_v2_1.0_224_quant.tflite\u0026quot;); BandEngineRegisterModel(engine, model); // 4. Create input and output tensors for the model. BandTensor* input_tensor = BandEngineCreateInputTensor(engine, model, 0); BandTensor* output_tensor = BandEngineCreateOutputTensor(engine, model, 0); std::tuple\u0026lt;unsigned char*, int, int\u0026gt; image_buffer = LoadRGBImageRaw(\u0026quot;cat.jpg\u0026quot;); // 5. (Optional) Create a buffer and an image processor to initialize the // input tensor with the image data. BandBuffer* buffer = BandBufferCreate(); BandBufferSetFromRawData(buffer, std::get\u0026lt;0\u0026gt;(image_buffer), std::get\u0026lt;1\u0026gt;(image_buffer), std::get\u0026lt;2\u0026gt;(image_buffer), kBandRGB); BandImageProcessorBuilder* builder = BandImageProcessorBuilderCreate(); BandImageProcessor* processor = BandImageProcessorBuilderBuild(builder); BandImageProcessorProcess(processor, buffer, input_tensor); // 6. Run the model. BandEngineRequestSync(engine, model, \u0026amp;input_tensor, \u0026amp;output_tensor); // 7. Get the result (class index). unsigned char* output = static_cast\u0026lt;unsigned char*\u0026gt;(BandTensorGetData(output_tensor)); // should be 282 (tiger cat) for cat.jpg size_t class_index = ArgMax\u0026lt;unsigned char\u0026gt;(output, 1001); // 8. Clean up. BandTensorDelete(input_tensor); BandTensorDelete(output_tensor); BandImageProcessorBuilderDelete(builder); BandImageProcessorDelete(processor); delete[] std::get\u0026lt;0\u0026gt;(image_buffer); BandEngineDelete(engine); BandConfigDelete(config); } API Types # Band provides the following types in the C API:\nBandConfig: Configuration object for the engine. BandConfigBuilder: Builder for the configuration object. BandEngine: Engine object. BandModel: Model object that holds the model data. It must outlive the engine. BandTensor: Tensor object that holds the input/output data of a model. BandBuffer: Wrapper for any data buffer interchangable with BandTensor. BandImageProcessor: Image processor object that converts an image buffer to a tensor. BandImageProcessorBuilder: Builder for the image processor object. BandRequestHandle: Handle for an asynchronous request. API Functions (Engine) # BandEngineCreate # BandEngine* BandEngineCreate(BandConfig* config); Creates a BandEngine instance with the given configuration.\nBandEngineDelete # void BandEngineDelete(BandEngine* engine); Deletes the BandEngine instance.\nBandEngineRegisterModel # void BandEngineRegisterModel(BandEngine* engine, BandModel* model); Registers a model to the engine. The engine will load the model and allocate resources for it.\nBandEngingeGetNumInputTensors # int BandEngingeGetNumInputTensors(BandEngine* engine, BandModel* model); Returns the number of input tensors for the given model.\nBandEngingeGetNumOutputTensors # int BandEngingeGetNumOutputTensors(BandEngine* engine, BandModel* model); Returns the number of output tensors for the given model.\nBandEngineGetNumWorkers # int BandEngineGetNumWorkers(BandEngine* engine); Returns the number of workers in the engine.\nBandEngineGetWorkerDevice # BandDeviceFlag BandEngineGetWorkerDevice(BandEngine* engine, int worker_index); Returns the device flag (e.g., kBandCPU, kBandGPU, \u0026hellip;) of the worker at the given index.\nBandEngineCreateInputTensor # BandTensor* BandEngineCreateInputTensor(BandEngine* engine, BandModel* model, int index); Creates an input tensor for the given model. The tensor is allocated by the engine and must be deleted by the caller.\nBandEngineCreateOutputTensor # BandTensor* BandEngineCreateOutputTensor(BandEngine* engine, BandModel* model, int index); Creates an output tensor for the given model. The tensor is allocated by the engine and must be deleted by the caller.\nBandEngineRequestSync # BandStatus BandEngineRequestSync( BandEngine* engine, BandModel* model, BandTensor** input_tensors, BandTensor** output_tensors) Runs the model with the given input tensors and stores the result in the output tensors. The function blocks until the execution is finished. Returns kBandOk if the execution is successful.\nBandEngineRequestAsync # BandRequestHandle BandEngineRequestAsync( BandEngine* engine, BandModel* model, BandTensor** input_tensors); Runs the model with the given input tensors. The function returns immediately and the result will be stored in the output tensors when the execution is finished. Returns a handle to the request.\nBandEngineWait # BandStatus BandEngineWait(BandEngine* engine, BandRequestHandle handle, BandTensor** output_tensors, size_t num_outputs); Blocks until the request is finished. Returns kBandOk if the execution is successful and the result is stored in the output tensors.\nBandEngineSetOnEndRequest # void BandEngineSetOnEndRequest( BandEngine* engine, void (*on_end_invoke)(void* user_data, BandRequestHandle job_id, BandStatus status), void* user_data); Sets a callback function that will be invoked when a request is finished. The callback function will be invoked in the engine thread. The status is the status of the request.\nAPI Functions (Buffer) # Band provides a buffer type that can be used to hold data for a tensor. The buffer can be created from a raw data pointer. The buffer can be converted to a tensor using an image processor.\nImageProcessorBuilder is used to build an ImageProcessor. ImageProcessor defines a series of operations to be applied to a BandBuffer and convert it to a BandTensor.\nBy default, builder without any operation will create a ImageProcessor provides a direct mapping from BandBuffer to BandTensor without normalization. E.g., automated color space conversion, resize to the output tensor shape, and data type conversion.\nBandBufferCreate # BandBuffer* BandBufferCreate(); Creates a buffer instance.\nBandBufferDelete # void BandBufferDelete(BandBuffer* buffer); Deletes the buffer instance.\nBandBufferSetFromRawData # BandStatus BandBufferSetFromRawData(BandBuffer* buffer, const void* data, size_t width, size_t height, BandBufferFormat format); Sets the buffer from raw image data. Supported formats are:\nkBandRGB (3 channels - 8 bits per channel, interleaved) kBandRGBA (4 channels - 8 bits per channel, interleaved) kBandGRAY (1 channel - 8 bits per channel) kBandNV21 (YUV 4:2:0 - 8 bits per channel, interleaved) kBandNV12 (YUV 4:2:0 - 8 bits per channel, interleaved) kBandYV12 (YUV 4:2:0 - 8 bits per channel, planar) kBandYV21 (YUV 4:2:0 - 8 bits per channel, planar) BandBufferSetFromYUVData # BandStatus BandBufferSetFromYUVData(BandBuffer* buffer, const void* y_data, const void* u_data, const void* v_data, size_t width, size_t height, size_t row_stride_y, size_t row_stride_uv, size_t pixel_stride_uv, BandBufferFormat buffer_format); Sets the buffer from YUV data. Supported formats are:\nkBandNV21 (YUV 4:2:0 - 8 bits per channel, interleaved) kBandNV12 (YUV 4:2:0 - 8 bits per channel, interleaved) kBandYV12 (YUV 4:2:0 - 8 bits per channel, planar) kBandYV21 (YUV 4:2:0 - 8 bits per channel, planar) BandImageProcessorBuilderCreate # BandImageProcessorBuilder* BandImageProcessorBuilderCreate(); Creates an image processor builder instance.\nBandImageProcessorBuilderDelete # void BandImageProcessorBuilderDelete(BandImageProcessorBuilder* builder); Deletes the image processor builder instance.\nBandImageProcessorBuilderBuild # BandImageProcessor* BandImageProcessorBuilderBuild( BandImageProcessorBuilder* builder); Builds an image processor instance from the builder.\nBandAddOperator # BandStatus BandAddOperator(BandImageProcessorBuilder* b, BandImageProcessorBuilderField field, int count, ...); Adds an operator to the builder. The order of the operators will be the order of the operations applied to the input buffer. E.g., BandAddOperator(builder, BAND_IMAGE_PROCESSOR_CROP, 4, 0, 0, 100, 100); will crop the input buffer from (0, 0) to (100, 100). This will return kBandError if the given variadic arguments are invalid. Available operators are:\nBAND_IMAGE_PROCESSOR_CROP: Crops the input buffer. int x0, int y0, int x1, int y1 - crop from top-left corner, inclusive BAND_IMAGE_PROCESSOR_RESIZE: Resizes the input buffer. int width, int height - resize to a new size BAND_IMAGE_PROCESSOR_ROTATE: Rotates the input buffer. float angle - counter-clockwise, between 0 and 360 in multiples of 90 BAND_IMAGE_PROCESSOR_FLIP: Flips the input buffer. bool horizontal, bool vertical - flip horizontally and/or vertically BAND_IMAGE_PROCESSOR_CONVERT_COLOR_SPACE: Converts the color space of the input buffer. BandBufferFormat target_format - convert the color space BAND_IMAGE_PROCESSOR_NORMALIZE: Normalizes the input buffer. float mean, float std - normalize the input buffer BAND_IMAGE_PROCESSOR_DATA_TYPE_CONVERT: Converts the data type of the input buffer. No argument required Convert the data type to the output data type. E.g., convert from 8-bit RGB to 32-bit float RGB (tensor). BandImageProcessorProcess # BandStatus BandImageProcessorProcess(BandImageProcessor* image_processor, BandBuffer* buffer, BandTensor* target_tensor); Applies the image processor to the input buffer and stores the result in the target tensor. Returns kBandOk if the operation is successful.\nBandImageProcessorDelete # void BandImageProcessorDelete(BandImageProcessor* processor); Deletes the image processor instance.\n"
      }
    );
  index.add(
      {
        id: 11,
        href: "/docs/api/java/",
        title: "Java",
        description: "One page summary of how to use a Java API.",
        content: "Introduction # Band provides a Java API to support Android native applications. This document provides a quick overview of how to use the Java API.\nExample # The following example shows how to use the Java API to create an engine, register a model, create input and output tensors, and run the model. Link provides a complete example of how to use the Java API to run a model on an image.\nimport java.util.List; import java.util.Arrays; import java.util.ArrayList; import org.mrsnu.band.BackendType; import org.mrsnu.band.Band; import org.mrsnu.band.Buffer; import org.mrsnu.band.BufferFormat; import org.mrsnu.band.Config; import org.mrsnu.band.ConfigBuilder; import org.mrsnu.band.CpuMaskFlag; import org.mrsnu.band.Device; import org.mrsnu.band.Engine; import org.mrsnu.band.ImageProcessor; import org.mrsnu.band.ImageProcessorBuilder; import org.mrsnu.band.Model; import org.mrsnu.band.Request; import org.mrsnu.band.SchedulerType; import org.mrsnu.band.SubgraphPreparationType; import org.mrsnu.band.Tensor; Engine engine; ImageProcessor processor; Model classifier; List\u0026lt;Tensor\u0026gt; inputs = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Tensor\u0026gt; outputs = new ArrayList\u0026lt;\u0026gt;(); @Override protected void onCreate(Bundle savedInstanceState) { // 1. Load the Band library Band.init(); // 2. Create a configuration for the engine. b = new ConfigBuilder(); b.addPlannerLogPath(\u0026quot;/data/local/tmp/log.json\u0026quot;); b.addSchedulers( new SchedulerType[]{SchedulerType.HETEROGENEOUS_EARLIEST_FINISH_TIME}); b.addMinimumSubgraphSize(7); b.addSubgraphPreparationType(SubgraphPreparationType.MERGE_UNIT_SUBGRAPH); b.addCPUMask(CpuMaskFlag.ALL); b.addPlannerCPUMask(CpuMaskFlag.PRIMARY); b.addWorkers(new Device[]{Device.CPU, Device.GPU, Device.DSP, Device.NPU}); b.addWorkerNumThreads(new int[]{1, 1, 1, 1}); b.addWorkerCPUMasks(new CpuMaskFlag[]{CpuMaskFlag.ALL, CpuMaskFlag.ALL, CpuMaskFlag.ALL, CpuMaskFlag.ALL}); b.addSmoothingFactor(0.1f); b.addProfileDataPath(\u0026quot;/data/local/tmp/profile.json\u0026quot;); b.addOnline(true); b.addNumWarmups(1); b.addNumRuns(1); b.addAllowWorkSteal(true); b.addAvailabilityCheckIntervalMs(30000); b.addScheduleWindowSize(10); // 3. Create an engine with the configuration. Config config = b.build(); engine = new Engine(config); // 4. Create a model from a file and register it to the engine. classifier = new Model(BackendType.TFLITE, \u0026quot;/data/local/tmp/mobilenet_v2_1.0_224_quant.tflite\u0026quot;); engine.registerModel(classifier); // 5. Create input and output tensors. inputs.add(engine.createInputTensor(classifier, 0)); outputs.add(engine.createOutputTensor(classifier, 0)); // 6. Create an image processor to preprocess the input image. ImageProcessorBuilder processorBuilder = new ImageProcessorBuilder(); processorBuilder.addColorSpaceConvert(BufferFormat.RGB); processorBuilder.addResize(224, 224); processorBuilder.addDataTypeConvert(); processor = processorBuilder.build(); } // called when an image is available (e.g. from camera2 API) private void processImage(Image image) { // 7. Preprocess the image and run the model. final Buffer buffer = new Buffer(image.getPlanes(), image.getWidth(), image.getHeight(), BufferFormat.YV12); preprocessor.process(buffer, inputs.get(0)); // 8. Run the model. engine.requestSync(classifier, inputs, outputs); // 9. Postprocess the output. ByteBuffer rawResults = outputs.get(0).getData().order(ByteOrder.nativeOrder()); rawResults.rewind(); FloatBuffer results = rawResults.asFloatBuffer(); float[] resultArray = new float[results.remaining()]; int class_index = 0; float max = 0; for (int i = 0; i \u0026lt; results.remaining(); i++) { float value = results.get(i); if (value \u0026gt; max) { max = value; class_index = i; } } // 10. Use the result (e.g., class_index should be 282 (tiger cat) for cat image) } Java API classes # The Java API is composed of the following core classes:\nConfig ConfigBuilder Engine Model Tensor Quantization Request Optional classes for preprocessing:\nBuffer ImageProcessor ImageProcessorBuilder Enums:\nBackendType BufferFormat CpuMaskFlag DataType LogSeverity SchedulerType SubgraphPreparationType TensorType WorkerType We will cover Engine, Model, Tensor, and Buffer classes in detail in the following sections.\nAPI Functions (Engine) # Engine(Config config) # public Engine(Config config) Creates an engine with the given configuration.\nregisterModel # public void registerModel(Model model) Registers a model to the engine. The model must be registered before it can be used to run inference.\ngetNumInputTensors # public int getNumInputTensors(Model model) Returns the number of input tensors of the given model.\ngetNumOutputTensors # public int getNumOutputTensors(Model model) Returns the number of output tensors of the given model.\nrequestSync # public void requestSync(Model model, List\u0026lt;Tensor\u0026gt; inputTensors, List\u0026lt;Tensor\u0026gt; outputTensors) Runs the given model synchronously with the given input and output tensors. The input and output tensors must be created by the engine.\nrequestAsync # public Request requestAsync(Model model, List\u0026lt;Tensor\u0026gt; inputTensors) Runs the given model asynchronously with the given input tensors. The input tensors must be created by the engine. Returns a request object that can be used to wait for the result.\nrequestAsyncBatch # public List\u0026lt;Request\u0026gt; requestAsyncBatch(List\u0026lt;Model\u0026gt; models, List\u0026lt;List\u0026lt;Tensor\u0026gt;\u0026gt; inputTensorLists) Runs the given models asynchronously with the given input tensors. The input tensors must be created by the engine. Returns a list of request objects that can be used to wait for the results.\nwait # public void wait(Request request, List\u0026lt;Tensor\u0026gt; outputTensors) Waits for the given request to finish and copies the output tensors to the given output tensors. The output tensors must be created by the engine.\ncreateInputTensor # public Tensor createInputTensor(Model model, int index) Creates an input tensor for the given model and index. The input tensor can be used to run the model.\ncreateOutputTensor # public Tensor createOutputTensor(Model model, int index) Creates an output tensor for the given model and index. The output tensor can be used to run the model.\nAPI Functions (Model) # Model(BackendType backendType, String filePath) # public Model(BackendType backendType, String filePath) Creates a model from the given file path. The model should be registered to the engine before it can be used to run inference.\nModel(BackendType backendType, ByteBuffer modelBuffer) # public Model(BackendType backendType, ByteBuffer modelBuffer) Creates a model from the given byte buffer. The model should be registered to the engine before it can be used to run inference.\ngetSupportedBackends # public List\u0026lt;BackendType\u0026gt; getSupportedBackends() Returns a list of supported backends for the model.\nAPI Functions (Tensor) # getType # public DataType getType() Returns the data type of the tensor.\nsetType # public void setType(DataType dataType) Sets the data type of the tensor.\ngetData # public ByteBuffer getData() Returns the data buffer of the tensor.\nsetData # public void setData(ByteBuffer data) Sets the data buffer of the tensor.\ngetDims # public int[] getDims() Returns the dimensions of the tensor.\nsetDims # public void setDims(int[] dims) Sets the dimensions of the tensor.\ngetBytes # public int getBytes() Returns the number of bytes of the tensor.\ngetName # public String getName() Returns the name of the tensor.\ngetQuantization # public Quantization getQuantization() Returns the quantization of the tensor.\nsetQuantization # public void setQuantization(Quantization quantization) Sets the quantization of the tensor.\nAPI Functions (Buffer) # Buffer is a helper class to wrap multiple types of memory buffers (e.g., byte buffer, image planes) into a single class. It is used to preprocess input images before running inference. It does not deep copy the data, so the data must be outlived the buffer.\nBuffer(Tensor tensor) # public Buffer(Tensor tensor) Creates a buffer from the given tensor. The tensor must be created by the engine.\nBuffer(byte[] buffer, int width, int height, BufferFormat bufferFormat) # public Buffer(byte[] buffer, int width, int height, BufferFormat bufferFormat) Creates a buffer from the given byte buffer, width, height, and format. We currently support RGB, RGBA, and GRAYSCALE formats.\nBuffer(byte[][] yuvBytes, int width, int height, int yRowStride, int uvRowStride, int uvPixelStride, BufferFormat bufferFormat) # public Buffer(byte[][] yuvBytes, int width, int height, int yRowStride, int uvRowStride, int uvPixelStride, BufferFormat bufferFormat) Creates a buffer from the given YUV byte buffer, width, height, row strides, and format. We currently support YUV formats (NV21, NV12, YV12, YV21).\nBuffer(Image.Plane[] planes, int width, int height, BufferFormat format) # public Buffer(Image.Plane[] planes, int width, int height, BufferFormat format) Creates a buffer from the given planes, width, height, and format. We currently support YUV formats (NV21, NV12, YV12, YV21).\n"
      }
    );
  index.add(
      {
        id: 12,
        href: "/docs/api/unreal-plugin/",
        title: "Unreal Engine Plugin",
        description: "One page summary of how to use the Unreal Engine Plugin.",
        content: "Introduction # Band provides an official plugin for Unreal Engine. We support both Blueprint (visual scripting language of the UE) and C++ based interfaces.\nExample # We first demonstrate how to use the plugin with a simple example. Below code snippet is part of the mobile augmented-reality classification example built with the C++ interface of the plugin.\nLink to Example\nC++ # The main workflow of the example is as follows: Initialize the plugin and load the model.\nPreallocate the input and output tensors. Create a widget to display the camera image and the classification result. Start the camera (we use our own camera plugin to access the Android camera). // Called when the game starts or when spawned void ADetectionActor::BeginPlay() { DetectorInputTensors = DetectorModel-\u0026gt;AllocateInputTensors(); DetectorOutputTensors = DetectorModel-\u0026gt;AllocateOutputTensors(); TSubclassOf\u0026lt;UBandUIBase\u0026gt; WidgetClassType = WidgetClass.LoadSynchronous(); Widget = CreateWidget\u0026lt;UBandUIBase\u0026gt;(GetWorld(), WidgetClassType); Widget-\u0026gt;AddToViewport(); CameraImage = Cast\u0026lt;UImage\u0026gt;(Widget-\u0026gt;GetWidgetFromName(\u0026quot;CameraImage\u0026quot;)); GetGameInstance()-\u0026gt;GetSubsystem\u0026lt;UBandSubSystem\u0026gt;()-\u0026gt;OnEndInvoke.AddUObject( this, \u0026amp;ADetectionActor::OnEndRequest); AndroidCamera-\u0026gt;StartCamera(640, 640, 30); AndroidCamera-\u0026gt;OnFrameAvailable.AddUObject( this, \u0026amp;ADetectionActor::OnFrameAvailable); AndroidCamera-\u0026gt;OnTextureAvailableDynamic.AddDynamic( this, \u0026amp;ADetectionActor::OnTextureAvailable); Super::BeginPlay(); } When a new frame is available, we feed the frame to the model and request the inference.\nvoid ADetectionActor::OnFrameAvailable(const UAndroidCameraFrame *Frame) { DetectorInputTensors[0]-\u0026gt;FromCameraFrame(Frame, 0.f, 1.f); FBandModule::Get().RequestAsync(DetectorModel, DetectorInputTensors); } void ADetectionActor::OnTextureAvailable(UTexture2D *Texture) { CameraImage-\u0026gt;SetBrushFromTexture(Texture, true); } When the inference is done, we get the result and update the widget.\nvoid ADetectionActor::OnEndRequest(int32 JobId, EBandStatus Status) { if (Status == EBandStatus::Ok) { UBandBlueprintLibrary::GetOutputs(JobId, DetectorOutputTensors); TArray\u0026lt;FBandBoundingBox\u0026gt; BoundingBoxes = UBandBlueprintLibrary::GetDetectedBoxes( DetectorOutputTensors, EBandDetector::SSDMNetV2, Label); Widget-\u0026gt;BoundingBoxes = BoundingBoxes; } else { UE_LOG(LogTemp, Error, TEXT(\u0026quot;Detection failed\u0026quot;)); } } "
      }
    );
  index.add(
      {
        id: 13,
        href: "/docs/",
        title: "Docs",
        description: "Docs Doks.",
        content: ""
      }
    );
  search.addEventListener('input', show_results, true);

  function show_results(){
    const maxResult = 5;
    var searchQuery = this.value;
    var results = index.search(searchQuery, {limit: maxResult, enrich: true});

    // flatten results since index.search() returns results for each indexed field
    const flatResults = new Map(); // keyed by href to dedupe results
    for (const result of results.flatMap(r => r.result)) {
      if (flatResults.has(result.doc.href)) continue;
      flatResults.set(result.doc.href, result.doc);
    }

    suggestions.innerHTML = "";
    suggestions.classList.remove('d-none');

    // inform user that no results were found
    if (flatResults.size === 0 && searchQuery) {
      const noResultsMessage = document.createElement('div')
      noResultsMessage.innerHTML = `No results for "<strong>${searchQuery}</strong>"`
      noResultsMessage.classList.add("suggestion__no-results");
      suggestions.appendChild(noResultsMessage);
      return;
    }

    // construct a list of suggestions
    for(const [href, doc] of flatResults) {
        const entry = document.createElement('div');
        suggestions.appendChild(entry);

        const a = document.createElement('a');
        a.href = href;
        entry.appendChild(a);

        const title = document.createElement('span');
        title.textContent = doc.title;
        title.classList.add("suggestion__title");
        a.appendChild(title);

        const description = document.createElement('span');
        description.textContent = doc.description;
        description.classList.add("suggestion__description");
        a.appendChild(description);

        suggestions.appendChild(entry);

        if(suggestions.childElementCount == maxResult) break;
    }
  }
}());
